%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[energies]{Definitions/mdpi} 
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\setlength{\headheight}{20.0pt}
\usepackage{bm}
\usepackage{pdflscape}
\usepackage{mathrsfs}
\usepackage{tabularx}
\usepackage{float}
%\documentclass[preprints,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

% Below journals will use APA reference format:
% admsci, aieduc, behavsci, businesses, econometrics, economies, education, ejihpe, famsci, games, humans, ijcs, ijfs, jintelligence, journalmedia, jrfm, jsam, languages, peacestud, psycholint, publications, tourismhosp, youth

% Below journals will use Chicago reference format:
% arts, genealogy, histories, humanities, laws, literature, religions, risks, socsci

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% accountaudit, acoustics, actuators, addictions, adhesives, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, aichem, aieduc, aieng, aimater, aimed, aipa, air, aisens, algorithms, allergies, alloys, amh, analog, analytica, analytics, anatomia, anesthres, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, appliedphys, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronautics, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, bioresourbioprod, biosensors, biosphere, biotech, birds, blockchains, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardio, cardiogenetics, cardiovascmed, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinbioenerg, clinpract, clockssleep, cmd, cmtr, coasts, coatings, colloids, colorants, commodities, complexities, complications, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryo, cryptography, crystals, csmf, ctn, culture, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, dhi, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecm, ecologies, econometrics, economies, edm, education, eesp, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, entomology, entropy, environments, environremediat, epidemiologia, epigenomes, esa, est, famsci, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fossstud, foundations, fractalfract, fuels, future, futureinternet, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gastronomy, gels, genealogy, genes, geographies, geohazards, geomatics, geometry, geosciences, geotechnics, geriatrics, germs, glacies, grasses, green, greenhealth, gucdd, hardware, hazardousmatters, healthcare, hearts, hemato, hematolrep, hep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hydropower, hygiene, idr, iic, ijcs, ijem, ijerph, ijfs, ijgi, ijmd, ijms, ijns, ijom, ijpb, ijt, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jaestheticmed, jal, jcdd, jcm, jcp, jcrm, jcs, jcto, jdad, jdb, jdream, jemr, jeta, jfb, jfmk, jgbg, jgg, jimaging, jintelligence, jlpea, jmahp, jmmp, jmms, jmp, jmse, jne, jnt, jof, joi, joitmc, joma, jop, joptm, jor, journalmedia, jox, jpbi, jphytomed, jpm, jrfm, jsam, jsan, jtaer, jvd, jzbg, kidneydial, kinasesphosphatases, knowledge, labmed, laboratories, lae, land, languages, laws, life, lights, limnolrev, lipidology, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrics, metrology, micro, microarrays, microbiolres, microelectronics, micromachines, microorganisms, microplastics, microwave, minerals, mining, mmphys, modelling, molbank, molecules, mps, msf, mti, multimedia, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, ndt, network, neuroglia, neuroimaging, neurolint, neurosci, nitrogen, notspecified, nursrep, nutraceuticals, nutrients, obesities, occuphealth, oceans, ohbm, onco, optics, oral, organics, organoids, osteology, oxygen, pandemics, parasites, parasitologia, particles, pathogens, pathophysiology, peacestud, pediatrrep, pets, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, populations, poultry, powders, precipitation, precisoncol, preprints, proceedings, processes, prosthesis, proteomes, psf, psychiatryint, psychoactives, psycholint, publications, purification, quantumrep, quaternary, qubs, radiation, rdt, reactions, realestate, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, rjpm, robotics, rsee, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, shi, signals, sinusitis, siuj, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stratsediment, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, tae, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, therapeutics, thermo, timespace, tomography, tourismhosp, toxics, toxins, tph, transplantology, transportation, traumacare, traumas, tri, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, welding, wem, wevj, wild, wind, women, world, youth, zoonoticdis

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, benchmark, book, bookreview, briefcommunication, briefreport, casereport, changes, clinicopathologicalchallenge, comment, commentary, communication, conceptpaper, conferenceproceedings, correction, conferencereport, creative, datadescriptor, discussion, entry, expressionofconcern, extendedabstract, editorial, essay, erratum, fieldguide, hypothesis, interestingimages, letter, meetingreport, monograph, newbookreceived, obituary, opinion, proceedingpaper, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, supfile, systematicreview, technicalnote, viewpoint, guidelines, registeredreport, tutorial,  giantsinurology, urologyaroundtheworld
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. Remove "pdftex" for (1) compiling with LaTeX & dvi2pdf (if eps figures are used) or for (2) compiling with XeLaTeX.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2026}
\copyrightyear{2025}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers: "Retracted: XXX" date in the original paper.
%\doinum{} % Used for some special journals, like molbank
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part
%\IsAssociation{yes} % For association journals

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{GridFM: A Physics-Informed Foundation Model for Multi-Task Energy Forecasting Using Real-Time NYISO Data}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0003-2145-1671} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-0000-000X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Ali Sayghe  $^{1}$\orcidA{}, Mohammed Mousa $^{1}$ and Salem Batiyah $^{1}$}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Ali Sayghe, Mohammed Mousa and Salem Batiyah}

%\longauthorlist{yes}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Department of Electrical Engineering, Yanbu Industrial College, Yanbu Saudi Arabia}

% Contact information of the corresponding author
\corres{Correspondence: A.S: Sayghea@rcjy.edu.sa; M.M: mousam1@RCJY.EDU.SA; S.B: batiyahs@RCJY.EDU.SA}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation.}  
% Current address should not be the same as any items in the Affiliation section.

%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes.

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{The rapid integration of renewable energy sources and increasing complexity of modern power grids demand advanced forecasting tools capable of simultaneously predicting multiple interconnected variables. While time series foundation models (TSFMs) have demonstrated remarkable zero-shot forecasting capabilities across diverse domains, their application to power grid operations remains limited due to complex coupling relationships between load, price, emissions, and renewable generation. This paper proposes GridFM, a novel physics-informed foundation model specifically designed for multi-task energy forecasting in power systems. GridFM introduces four key innovations: (1) a FreqMixer adaptation layer that transforms pre-trained foundation model representations to power-grid-specific patterns through frequency domain mixing without modifying base weights; (2) a physics-informed constraint module embedding power balance equations and zonal grid topology using graph neural networks; (3) a multi-task learning framework enabling joint forecasting of load demand, locational-based marginal prices (LBMP), carbon emissions, and renewable generation with uncertainty-weighted loss functions; and (4) an explainability module utilizing SHAP values and attention visualization for interpretable predictions. We validate GridFM using over 10 years of real-time data from the New York Independent System Operator (NYISO) at 5-minute resolution, comprising more than 10 million data points across 11 load zones. Comprehensive experiments demonstrate that GridFM achieves state-of-the-art performance with 18.5\% improvement in load forecasting MAPE (achieving 2.14\%), 23.2\% improvement in price forecasting (achieving 7.8\% MAPE), and 21.7\% improvement in emission prediction compared to existing TSFMs including Chronos, TimesFM, and Moirai-MoE. Ablation studies confirm the contribution of each proposed component. The physics-informed constraints reduce physically inconsistent predictions by 67\%, while the multi-task framework improves individual task performance by exploiting inter-variable correlations. The proposed model provides interpretable predictions supporting the Climate Leadership and Community Protection Act (CLCPA) 2030/2040 compliance objectives, enabling grid operators to make informed decisions for sustainable energy transition and carbon reduction strategies.
}

% Keywords
\keyword{foundation model; time series forecasting; power grid; NYISO; multi-task learning; physics-informed neural network; deep learning; transformer; mixture of experts; explainable AI; renewable energy; carbon emissions} 

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data
%\dataset{DOI number or link to the deposited data set if the data set is published separately. If the data set shall be published as a supplement to this paper, this field will be filled by the journal editors. In this case, please submit the data set as a supplement.}
%\datasetlicense{License under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal BioTech, Fishes, Neuroimaging and Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{For entry manuscripts only: please provide a brief overview of the entry title instead of an abstract.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Different journals have different requirements. Please check the specific journal guidelines in the "Instructions for Authors" on the journal's official website.
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%
%
%\noindent The goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What are the implications of the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:introduction}

\subsection{Background and Motivation}

The global energy sector is undergoing a fundamental transformation driven by the imperative to reduce carbon emissions and integrate renewable energy sources into existing power infrastructure~\cite{IEA2024,IPCC2023}. This energy transition introduces unprecedented challenges for grid operators who must balance supply and demand while maintaining system reliability under increasingly uncertain conditions~\cite{Mohammadi2024,Ahmed2024}. The New York Independent System Operator (NYISO), which manages the electric grid serving over 19 million people, exemplifies these challenges while operating under the Climate Leadership and Community Protection Act (CLCPA). This landmark legislation mandates 70\% renewable electricity by 2030 and 100\% zero-emission electricity by 2040~\cite{NYISO2024,CLCPA2019}, requiring sophisticated forecasting tools that can simultaneously predict multiple interconnected variables: load demand, electricity prices, carbon emissions, and renewable generation.

Accurate forecasting of these grid variables is essential for efficient power system operations~\cite{Hong2020,Haben2021}. Load forecasting enables optimal unit commitment and economic dispatch decisions~\cite{Hammad2020}. Price forecasting supports market participants in developing bidding strategies and managing financial risks~\cite{Weron2014}. Emission forecasting is increasingly critical for carbon accounting and regulatory compliance~\cite{Lindberg2021}. Renewable generation forecasting addresses the inherent intermittency of wind and solar resources~\cite{Wang2022Review}. Traditional forecasting approaches typically treat these variables independently, failing to capture the complex coupling relationships that exist in integrated energy systems~\cite{Li2024Joint}. For instance, high renewable generation periods often correlate with lower electricity prices and reduced emissions, while peak load conditions influence both pricing dynamics and generation dispatch decisions~\cite{Lago2021}.

\textcolor{red}{To illustrate these coupling relationships concretely, consider a scenario where wind power generation surges unexpectedly. Through the merit-order effect, this zero-marginal-cost generation displaces higher-cost fossil fuel units in the dispatch stack, causing electricity prices to decrease---sometimes dramatically, even to negative values during periods of oversupply. Simultaneously, because fossil fuel generation is curtailed, carbon emissions per MWh decline as the generation mix shifts toward cleaner sources. Conversely, during a summer heat wave when air conditioning drives peak load demand, utilities must dispatch expensive peaking plants (often natural gas turbines), which increases both electricity prices and emission rates. These interconnected dynamics demonstrate why accurate multi-task forecasting must capture the physical and economic coupling between load, price, emissions, and renewable generation rather than treating each variable in isolation.}

Recent advances in deep learning have significantly improved forecasting accuracy for individual grid variables. Long Short-Term Memory (LSTM) networks~\cite{Hochreiter1997}, Gated Recurrent Units (GRU)~\cite{Cho2014}, Temporal Convolutional Networks (TCN)~\cite{Bai2018}, and Transformer architectures~\cite{Vaswani2017} have demonstrated superior performance compared to traditional statistical methods such as ARIMA and exponential smoothing~\cite{Haben2019,Hewamalage2021}. Temporal Fusion Transformers (TFT)~\cite{Lim2021} introduced interpretable attention mechanisms for multi-horizon forecasting. Informer~\cite{Zhou2021Informer} and Autoformer~\cite{Wu2021Autoformer} addressed the computational complexity of long-sequence forecasting. However, these approaches require task-specific training and extensive labeled data, limiting their generalization capabilities across different forecasting horizons, grid conditions, and geographic regions~\cite{Oreshkin2021}.

\subsection{Time Series Foundation Models}

The emergence of time series foundation models (TSFMs) represents a paradigm shift in forecasting methodology~\cite{Liang2024Survey,Jin2024Survey}. Inspired by the remarkable success of large language models (LLMs) in natural language processing~\cite{Brown2020GPT3,Touvron2023LLaMA}, TSFMs leverage transfer learning by pre-training on massive time series datasets from diverse domains, enabling zero-shot or few-shot predictions on previously unseen data without task-specific training~\cite{Das2024TimesFM,Ansari2024Chronos}. This capability addresses a fundamental limitation of traditional deep learning approaches: the need for extensive domain-specific training data and computational resources for each new forecasting task.

Several prominent TSFMs have emerged in recent years. TimesFM~\cite{Das2024TimesFM}, developed by Google Research, represents a decoder-only transformer designed specifically for time series, utilizing patching mechanisms to capture local temporal patterns. Chronos~\cite{Ansari2024Chronos}, developed by Amazon, introduced a tokenization framework that discretizes continuous time series values into a fixed vocabulary of 4,096 tokens. Moirai~\cite{Woo2024Moirai}, developed by Salesforce AI Research, introduces an encoder-only architecture with any-variate attention mechanisms. The recent Moirai-MoE~\cite{Liu2024MoiraiMoE} incorporates sparse mixture-of-experts (MoE) layers, achieving token-level model specialization. Additional notable TSFMs include Lag-Llama~\cite{Rasul2024}, Time-MoE~\cite{Shi2024TimeMoE}, and MOMENT~\cite{Goswami2024MOMENT}.

\subsection{Research Gaps and Challenges}

Despite the impressive capabilities demonstrated by existing TSFMs, their application to power grid forecasting faces several critical challenges:

\textbf{Gap 1: Domain Specificity.} TSFMs are pre-trained on heterogeneous datasets spanning diverse domains including retail, weather, traffic, and economics~\cite{Liang2024Survey}. While this diversity enables broad generalization, power grids exhibit unique characteristics: strong daily and weekly periodicities driven by human activity patterns, complex spatial dependencies across interconnected zones, and fundamental physical constraints governing energy balance. General-purpose TSFMs cannot exploit these power-grid-specific patterns without domain adaptation.

\textbf{Gap 2: Multi-Task Integration.} Existing TSFMs primarily focus on univariate forecasting or independent multivariate predictions~\cite{Das2024TimesFM,Ansari2024Chronos}. They fail to exploit the complex coupling relationships between grid variables. Load demand drives generation requirements; generation mix determines both prices and emissions; renewable variability affects reserve requirements. A truly effective grid forecasting model should capture and exploit these interdependencies.

\textbf{Gap 3: Physics Constraints.} Power systems are governed by fundamental physical laws including Kirchhoff's laws, power balance requirements, and transmission capacity limits~\cite{Chatzivasileiadis2020PINN}. Existing TSFMs operate as purely data-driven black boxes, potentially generating predictions that violate physical constraints and are therefore operationally infeasible.

\textbf{Gap 4: Explainability.} Grid operations represent critical infrastructure where forecasting errors can lead to blackouts, equipment damage, or significant financial losses~\cite{Hong2020}. Operators require not just accurate predictions but also understanding of the factors driving those predictions. Existing TSFMs largely lack interpretability mechanisms suitable for operational decision support.

\textbf{Gap 5: Policy Alignment.} Energy forecasting must increasingly support policy objectives related to decarbonization and sustainability~\cite{CLCPA2019}. The ability to forecast carbon emissions alongside traditional variables enables real-time carbon accounting and supports informed decision-making for emission reduction.

\subsection{Research Objectives and Contributions}

To address these critical gaps, we propose \textbf{GridFM}, a physics-informed foundation model specifically designed for multi-task energy forecasting in power systems. The main contributions are:

\begin{enumerate}
    \item \textbf{Novel Architecture:} We introduce GridFM, the first foundation model specifically adapted for multi-task power grid forecasting, with comprehensive validation across multiple ISO regions (NYISO, PJM, CAISO).
    
    \item \textbf{FreqMixer Adaptation Layer:} We propose a novel frequency-domain mixing mechanism that adapts general TSFM representations to power-grid-specific temporal patterns, with grid-specific initialization and quantitative validation of learned frequency responses.
    
    \item \textbf{Physics-Informed Constraint Module:} We develop a constraint module that embeds power system physics including generation-load balance equations with DC power flow approximation and zonal topology encoding through graph neural networks.
    
    \item \textbf{Multi-Task Learning Framework:} We design a joint forecasting framework for simultaneous prediction of load, LBMP, carbon emissions, and renewable generation with adaptive coupling constraints that learn time-varying relationships.
    
    \item \textbf{Rigorous Evaluation:} We conduct extensive experiments using over 10 years of real-time NYISO data with additional validation on PJM and CAISO, employing rolling-origin cross-validation with five folds and statistical significance testing against both zero-shot and fine-tuned foundation model baselines.
    
    \item \textbf{Explainability Module:} We integrate SHAP-based feature attribution and attention visualization with deployment considerations for grid operators.
    
    \item \textbf{Open-Source Release:} We provide complete code, pre-trained models, and preprocessing scripts at \url{https://github.com/GridFM/GridFM}\textcolor{red}{. The repository has been made publicly accessible and includes comprehensive documentation, installation instructions, usage examples, and scripts for reproducing all experimental results presented in this paper.}
\end{enumerate}

The remainder of this paper is organized as follows: Section~\ref{sec:related_work} reviews related work. Section~\ref{sec:methodology} presents the GridFM architecture. Section~\ref{sec:experiments} describes the experimental setup. Section~\ref{sec:results} presents experimental results. Section~\ref{sec:discussion} discusses findings and limitations. Section~\ref{sec:conclusion} concludes the paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 2: RELATED WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}\label{sec:related_work}

This section reviews relevant literature in three key areas: time series foundation models, deep learning for power grid forecasting, and physics-informed neural networks for energy systems.

\subsection{Time Series Foundation Models}

The development of foundation models for time series analysis has accelerated dramatically since 2023~\cite{Liang2024Survey,Jin2024Survey}. We categorize existing approaches into three main paradigms.

\subsubsection{Language Model Adaptation Approaches}

LLMTime~\cite{Gruver2024LLMTime} demonstrated that GPT-3 and LLaMA can perform zero-shot forecasting by treating numerical time series as text sequences. PromptCast~\cite{Xue2023PromptCast} introduced prompt-based forecasting using natural language descriptions. GPT4TS~\cite{Zhou2024GPT4TS} proposed freezing pre-trained GPT-2 weights and adding lightweight adaptation layers for time series tasks, achieving competitive performance with minimal fine-tuning.

\subsubsection{Tokenization-Based Native Models}

Chronos~\cite{Ansari2024Chronos} introduced a principled tokenization framework specifically designed for time series forecasting, discretizing values into bins and training T5-style encoder-decoder models. TimeGPT~\cite{Garza2024TimeGPT} represents the first commercial time series foundation model, offering zero-shot forecasting through an API service. Lag-Llama~\cite{Rasul2024} combines tokenization with explicit lag features for improved probabilistic forecasting.

\subsubsection{Native Time Series Architectures}

TimesFM~\cite{Das2024TimesFM} introduced a decoder-only transformer architecture designed specifically for time series, utilizing patching mechanisms similar to Vision Transformers. Moirai~\cite{Woo2024Moirai} introduced the Universal Time Series Forecasting Transformer (UNITS) architecture with any-variate attention, enabling flexible handling of multivariate time series with varying dimensions. Moirai-MoE~\cite{Liu2024MoiraiMoE} extends this with sparse mixture-of-experts layers, achieving token-level model specialization through top-k routing. Time-MoE~\cite{Shi2024TimeMoE} scales the MoE approach to billions of parameters with hierarchical expert organization.

Table~\ref{tab:tsfm_comparison} provides a comprehensive comparison of existing TSFMs.

\begin{table}[H]
\caption{Comparison of time series foundation models.\label{tab:tsfm_comparison}}
\centering
\small
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Model} & \textbf{Type} & \textbf{Params} & \textbf{Multi.} & \textbf{Exog.} & \textbf{Prob.} & \textbf{Open} \\
\midrule
TimesFM~\cite{Das2024TimesFM} & Decoder & 200M & -- & -- & -- & \checkmark \\
Chronos~\cite{Ansari2024Chronos} & Enc-Dec & 8--710M & -- & -- & \checkmark & \checkmark \\
Moirai~\cite{Woo2024Moirai} & Encoder & 14--311M & \checkmark & \checkmark & \checkmark & \checkmark \\
Moirai-MoE~\cite{Liu2024MoiraiMoE} & Sparse MoE & 11--117M & \checkmark & \checkmark & \checkmark & \checkmark \\
Time-MoE~\cite{Shi2024TimeMoE} & Sparse MoE & 1--2.4B & \checkmark & \checkmark & \checkmark & \checkmark \\
Lag-Llama~\cite{Rasul2024} & Decoder & 7--70M & -- & -- & \checkmark & \checkmark \\
\midrule
\textbf{GridFM (Ours)} & \textbf{Hybrid MoE} & \textbf{135M} & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Deep Learning for Power Grid Forecasting}

Short-term load forecasting (STLF) has been extensively studied using deep learning approaches~\cite{Hong2020,Haben2021}. LSTM networks~\cite{Kong2019LSTM}, hybrid CNN-LSTM architectures~\cite{Kim2019CNNLSTM}, and Temporal Fusion Transformers~\cite{Lim2021} have shown strong performance on various utility datasets. Electricity price forecasting presents unique challenges due to high volatility, occasional negative prices, and regime changes~\cite{Weron2014,Lago2021Review}. Yang et al.~\cite{Yang2024ATTnet} proposed ATTnet for NYISO real-time price prediction, achieving competitive accuracy through gated recurrent units with attention. Multi-task learning approaches~\cite{Li2024Joint,Chen2024MTL} have begun to exploit relationships between related forecasting tasks, though typically limited to pairs of variables.

\subsection{Physics-Informed Neural Networks for Power Systems}

Physics-informed neural networks (PINNs)~\cite{Raissi2019PINN} embed domain knowledge as constraints in the learning process, ensuring predictions respect known physical laws. Chatzivasileiadis et al.~\cite{Chatzivasileiadis2020PINN} pioneered the application of PINNs to power flow analysis, demonstrating that physics constraints improve both accuracy and physical consistency. Donon et al.~\cite{Donon2020GNN} introduced graph neural networks for power grids, encoding network topology explicitly. Hossain et al.~\cite{Hossain2025} developed an interpretable PINN framework for energy consumption prediction achieving $R^2 = 0.9972$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 3: METHODOLOGY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methodology}\label{sec:methodology}

This section presents the GridFM architecture in detail. We begin with the problem formulation, then describe each architectural component.

\subsection{Problem Formulation}\label{sec:problem}

Let $\mathcal{D} = \{(\mathbf{X}_t, \mathbf{Y}_t)\}_{t=1}^{T}$ denote the historical dataset where $\mathbf{X}_t \in \mathbb{R}^{L \times D}$ represents the input features with context length $L$ and feature dimension $D$, and $\mathbf{Y}_t \in \mathbb{R}^{H \times K}$ represents the target variables with prediction horizon $H$ and $K$ forecasting tasks.

The multi-task target matrix is defined as:
\begin{equation}\label{eq:target_matrix}
    \mathbf{Y}_t = \begin{bmatrix} 
        \mathbf{y}_t^{\text{load}} \\ 
        \mathbf{y}_t^{\text{price}} \\ 
        \mathbf{y}_t^{\text{emission}} \\ 
        \mathbf{y}_t^{\text{renewable}} 
    \end{bmatrix} \in \mathbb{R}^{K \times H}
\end{equation}

The objective is to learn a mapping $f_\theta: \mathbb{R}^{L \times D} \rightarrow \mathbb{R}^{H \times K}$ that minimizes:
\begin{equation}\label{eq:total_loss}
    \boxed{\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{MTL}} + \lambda_p \mathcal{L}_{\text{physics}} + \lambda_c \mathcal{L}_{\text{coupling}}}
\end{equation}

\subsection{GridFM Architecture Overview}\label{sec:architecture}

Figure~\ref{fig:architecture} illustrates the complete GridFM architecture, which comprises five main modules organized in a hierarchical structure:

\begin{enumerate}
    \item \textbf{Input Embedding Layer:} Projects raw features to model dimension with positional and temporal encodings
    \item \textbf{FreqMixer Adaptation Layer:} Transforms representations to power-grid-specific patterns through frequency domain mixing
    \item \textbf{Foundation Model Backbone:} Pre-trained transformer with frozen weights and LoRA adapters for general time series representations
    \item \textbf{Physics-Informed Constraint Module:} Embeds power balance equations and zonal grid topology via GCN
    \item \textbf{Multi-Task Output Heads:} Task-specific prediction layers with explainability features
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{fig1_architecture(1).pdf}
\caption{GridFM architecture overview. The model comprises five main modules: (1) Input Embedding with temporal and positional encodings; (2) FreqMixer Adaptation Layer for power-grid-specific pattern learning through frequency domain processing; (3) Pre-trained Foundation Model Backbone (Moirai-MoE) with frozen weights and LoRA adapters; (4) Physics-Informed Constraint Module embedding power balance equations and zonal grid topology; (5) Multi-Task Output Heads for joint forecasting of load, price, emission, and renewable generation. The total loss combines multi-task, physics, and coupling components.\label{fig:architecture}}
\end{figure}

\subsection{Input Embedding and Positional Encoding}\label{sec:embedding}

Given input sequence $\mathbf{X} \in \mathbb{R}^{L \times D}$, we apply a linear projection:
\begin{equation}\label{eq:input_embedding}
    \mathbf{E}^{(0)} = \mathbf{X}\mathbf{W}_{\text{in}} + \mathbf{b}_{\text{in}}
\end{equation}

We employ sinusoidal positional encoding~\cite{Vaswani2017}:
\begin{equation}\label{eq:positional_encoding}
    \begin{aligned}
        \text{PE}_{(\text{pos}, 2i)} &= \sin\left(\frac{\text{pos}}{10000^{2i/d}}\right) \\
        \text{PE}_{(\text{pos}, 2i+1)} &= \cos\left(\frac{\text{pos}}{10000^{2i/d}}\right)
    \end{aligned}
\end{equation}

Additionally, we incorporate cyclic temporal encodings to capture grid-specific periodicities:
\begin{equation}\label{eq:temporal_encoding}
    \begin{aligned}
        \mathbf{T}_{\text{hour}}(h) &= \left[\sin\left(\frac{2\pi h}{24}\right), \cos\left(\frac{2\pi h}{24}\right)\right] \\
        \mathbf{T}_{\text{day}}(d) &= \left[\sin\left(\frac{2\pi d}{7}\right), \cos\left(\frac{2\pi d}{7}\right)\right] \\
        \mathbf{T}_{\text{year}}(m) &= \left[\sin\left(\frac{2\pi m}{12}\right), \cos\left(\frac{2\pi m}{12}\right)\right]
    \end{aligned}
\end{equation}

\subsection{FreqMixer Adaptation Layer}\label{sec:freqmixer}

The FreqMixer module represents a key innovation in GridFM, enabling adaptation of general foundation model representations to power-grid-specific temporal patterns. Unlike prior frequency-domain methods such as FEDformer~\cite{zhou2022fedformer}, FreqMixer introduces learnable masks specifically initialized to emphasize grid-relevant frequencies. Figure~\ref{fig:freqmixer} illustrates the detailed architecture of this layer.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Fig/fig2_freqmixer.pdf}
\caption{Detailed architecture of the FreqMixer Adaptation Layer. The input embeddings $\mathbf{E} \in \mathbb{R}^{L \times d}$ are transformed to the frequency domain via Fast Fourier Transform (FFT), filtered through a learnable frequency mask $\sigma(\mathbf{M})$, processed by a frequency mixing MLP, and transformed back via Inverse FFT (IFFT). A residual connection preserves the original signal, followed by layer normalization. The inset shows an example of the learned frequency mask that selectively amplifies power-grid-relevant periodicities (12h, 24h, 168h).\label{fig:freqmixer}}
\end{figure}

\subsubsection{Spectral Decomposition}

Given embedded sequence $\mathbf{E} \in \mathbb{R}^{L \times d}$, we apply the Discrete Fourier Transform (DFT):
\begin{equation}\label{eq:dft}
    \hat{\mathbf{E}}_k = \sum_{n=0}^{L-1} \mathbf{E}_n \cdot \exp\left(-\frac{i2\pi kn}{L}\right), \quad k = 0, 1, \ldots, \lfloor L/2 \rfloor
\end{equation}

\subsubsection{Learnable Frequency Mask with Grid-Specific Initialization}

We introduce a learnable frequency mask $\mathbf{M} \in \mathbb{R}^{N_f \times d}$ with initialization that emphasizes expected grid periodicities:
\begin{equation}\label{eq:frequency_mask}
    \tilde{\mathbf{E}}_k = \hat{\mathbf{E}}_k \odot \sigma(\mathbf{M}_k + \mathbf{b}_k)
\end{equation}

The mask is initialized as:
\begin{equation}\label{eq:mask_init}
    \mathbf{M}_k^{(0)} = \begin{cases}
        1.5 & \text{if } f_k \in \{f_{12h}, f_{24h}, f_{168h}\} \pm \delta \\
        1.0 & \text{otherwise}
    \end{cases}
\end{equation}
where $f_{12h}$, $f_{24h}$, and $f_{168h}$ correspond to 12-hour, daily, and weekly periodicities, and $\delta$ allows for slight frequency variations.

\textcolor{red}{The tolerance parameter $\delta$ is set to 2 frequency bins (i.e., $\delta = 2$) to account for slight variations in detected periodicities that arise from sampling rate effects and finite sequence length. Specifically, given a context length $L = 288$ (24 hours at 5-minute resolution), the frequency resolution is $\Delta f = 1/L$, and $\delta = 2$ allows the mask to capture periodicities within $\pm 2\Delta f$ of the target frequencies. Our sensitivity analysis shows that $\delta \in [1, 3]$ produces stable results: smaller values ($\delta = 0$) miss nearby periodicities due to spectral leakage, while larger values ($\delta > 4$) over-smooth the frequency response and lose selectivity. The choice of $\delta = 2$ balances precision in capturing grid-specific periodicities with robustness to minor frequency shifts caused by data preprocessing variations.}

\subsubsection{Frequency Mixing Network}

To capture cross-frequency dependencies:
\begin{equation}\label{eq:frequency_mixing}
    \bar{\mathbf{E}} = \text{MLP}_{\text{mix}}\left(\text{Flatten}(\tilde{\mathbf{E}})\right)
\end{equation}

\subsubsection{Inverse Transform and Residual Connection}

The final output includes a residual connection and layer normalization:
\begin{equation}\label{eq:freqmixer_output}
    \mathbf{E}^{\text{freq}} = \text{LayerNorm}(\text{Real}(\text{IFFT}(\bar{\mathbf{E}})) + \mathbf{E})
\end{equation}

Algorithm~\ref{alg:freqmixer} presents the complete FreqMixer forward pass.

\begin{algorithm}[H]
\caption{FreqMixer Adaptation Layer}\label{alg:freqmixer}
\begin{algorithmic}[1]
\REQUIRE Input embeddings $\mathbf{E} \in \mathbb{R}^{B \times L \times d}$, learnable mask $\mathbf{M} \in \mathbb{R}^{N_f \times d}$
\ENSURE Adapted embeddings $\mathbf{E}^{\text{freq}} \in \mathbb{R}^{B \times L \times d}$
\STATE $\hat{\mathbf{E}} \leftarrow \text{FFT}(\mathbf{E}, \text{dim}=1)$
\STATE $\mathbf{M}_{\sigma} \leftarrow \sigma(\mathbf{M} + \mathbf{b})$
\STATE $\tilde{\mathbf{E}} \leftarrow \hat{\mathbf{E}} \odot \mathbf{M}_{\sigma}$
\STATE $\mathbf{z} \leftarrow \text{Flatten}(\tilde{\mathbf{E}})$
\STATE $\bar{\mathbf{z}} \leftarrow \text{MLP}_{\text{mix}}(\mathbf{z})$
\STATE $\bar{\mathbf{E}} \leftarrow \text{Reshape}(\bar{\mathbf{z}})$
\STATE $\mathbf{E}' \leftarrow \text{IFFT}(\bar{\mathbf{E}}, \text{dim}=1)$
\STATE $\mathbf{E}^{\text{freq}} \leftarrow \text{LayerNorm}(\text{Real}(\mathbf{E}') + \mathbf{E})$
\RETURN $\mathbf{E}^{\text{freq}}$
\end{algorithmic}
\end{algorithm}

\subsection{Foundation Model Backbone}\label{sec:backbone}

GridFM leverages pre-trained time series foundation models as the backbone encoder. We primarily utilize Moirai-MoE~\cite{Liu2024MoiraiMoE} due to its sparse mixture-of-experts architecture, which provides efficient scaling and task specialization. \textcolor{red}{The design of our spatial dependency modeling through sparse expert routing draws inspiration from recent advances in spatio-temporal graph attention networks for renewable energy forecasting~\cite{Simeunovic2022TSMGAT}, which demonstrated effective learning of dynamic spatial dependencies that vary with operating conditions.}

\subsubsection{Sparse Mixture-of-Experts Layer}

The MoE architecture routes each input token to a subset of expert networks:
\begin{equation}\label{eq:moe}
    \text{MoE}(\mathbf{x}) = \sum_{i=1}^{N_e} g_i(\mathbf{x}) \cdot E_i(\mathbf{x})
\end{equation}

The gating weights are computed through a learned routing mechanism:
\begin{equation}\label{eq:gating}
    g(\mathbf{x}) = \text{Softmax}\left(\text{TopK}(\mathbf{W}_g \mathbf{x} + \epsilon, k)\right)
\end{equation}

\subsubsection{Any-Variate Attention}

Following Moirai~\cite{Woo2024Moirai}, we employ any-variate attention:
\begin{equation}\label{eq:attention}
    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}} + \mathbf{B}\right)\mathbf{V}
\end{equation}

\subsubsection{Low-Rank Adaptation (LoRA)}

To enable efficient fine-tuning while preserving pre-trained knowledge, we apply LoRA~\cite{Hu2022LoRA} to the attention layers:
\begin{equation}\label{eq:lora}
    \mathbf{W}' = \mathbf{W} + \alpha \cdot \mathbf{B}\mathbf{A}
\end{equation}
where $\mathbf{B} \in \mathbb{R}^{d \times r}$, $\mathbf{A} \in \mathbb{R}^{r \times d}$, $r \ll d$, and $\alpha$ is a scaling factor.

\subsection{Physics-Informed Constraint Module}\label{sec:physics}

The physics-informed module embeds power system constraints to ensure predictions respect fundamental physical laws.

\subsubsection{Improved Power Balance Constraint}

The fundamental power balance equation for each zone $z$\textcolor{red}{, extended to include energy storage systems}:
\begin{equation}\label{eq:power_balance}
    \sum_{g \in \mathcal{G}_z} P_g(t) \textcolor{red}{+ P_{\text{storage}}^z(t)} = P_{\text{load}}^z(t) + \sum_{l \in \mathcal{L}_z} P_{\text{flow}}^l(t) + P_{\text{loss}}^z(t)
\end{equation}
\textcolor{red}{where $P_{\text{storage}}^z(t)$ represents the net power injection from energy storage devices in zone $z$, which is positive during discharging (acting as generation) and negative during charging (acting as load). This term captures battery energy storage systems (BESS), pumped hydro storage, and other storage technologies that are increasingly prevalent in modern grid operations.}

The physics loss with DC power flow approximation:
\begin{equation}\label{eq:physics_loss}
    \mathcal{L}_{\text{balance}} = \frac{1}{H}\sum_{h=1}^{H} \left\| \hat{P}^{\text{gen}}_h - \hat{P}^{\text{load}}_h - \hat{P}_{\text{loss}}(\hat{P}^{\text{load}}_h, \mathcal{T}) \right\|_2^2
\end{equation}
where $\hat{P}_{\text{loss}}$ is computed using the zonal topology $\mathcal{T}$:
\begin{equation}\label{eq:loss_approx}
    \hat{P}_{\text{loss}} \approx \sum_{(i,j) \in \mathcal{E}} \frac{(\hat{\theta}_i - \hat{\theta}_j)^2}{X_{ij}}
\end{equation}

\textcolor{red}{The parameters in these equations have the following physical meanings:
\begin{itemize}
    \item $\hat{P}^{\text{gen}}_h$: Predicted total generation at hour $h$ (in MW), computed as the sum of forecasted renewable generation and fossil fuel generation from the respective task heads.
    \item $\hat{P}^{\text{load}}_h$: Predicted load demand at hour $h$ (in MW), output from the load forecasting task head.
    \item $\hat{\theta}_i, \hat{\theta}_j$: Predicted voltage phase angles at nodes (zones) $i$ and $j$ respectively (in radians), derived from the GCN spatial embedding layer.
    \item $X_{ij}$: Line reactance between nodes $i$ and $j$ (in per-unit on the system MVA base), obtained from NYISO transmission data. This represents the electrical ``distance'' between zones.
    \item $\mathcal{E}$: The set of transmission lines (edges) connecting adjacent zones in the grid topology.
\end{itemize}
This DC power flow approximation assumes lossless transmission lines and small angle differences, which is valid for bulk power system analysis where reactive power and voltage magnitude variations are secondary effects.}

\subsubsection{Zonal Topology Encoding}

We encode the zonal topology using a Graph Convolutional Network (GCN)~\cite{Kipf2017GCN}:
\begin{equation}\label{eq:gcn}
    \mathbf{H}^{(l+1)} = \sigma\left(\tilde{\mathbf{D}}^{-1/2}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-1/2}\mathbf{H}^{(l)}\mathbf{W}^{(l)}\right)
\end{equation}
where $\tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}$ is the adjacency matrix with self-loops, and $\tilde{\mathbf{D}}_{ii} = \sum_j \tilde{\mathbf{A}}_{ij}$.

\subsection{Multi-Task Learning Framework}\label{sec:mtl}

\textcolor{red}{Our multi-task learning framework builds upon recent advances in physics-informed hybrid multi-task architectures. In particular, the approach of combining physics constraints with shared representations for multi-output prediction has been successfully demonstrated in battery aging estimation~\cite{Zhang2025PIHMTL}, providing theoretical grounding for our integration of power system physics with multi-task forecasting.}

Figure~\ref{fig:mtl} illustrates the multi-task learning framework in GridFM.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{fig3_mtl_framework.pdf}
\caption{Multi-task learning framework in GridFM. The shared foundation model backbone processes input features with frozen pre-trained weights and LoRA adapters. Task-specific output heads generate predictions for load, price, emission, and renewable generation forecasting. The uncertainty-weighted loss $\mathcal{L}_{\text{MTL}}$ automatically balances task contributions using learned uncertainty parameters $\sigma_k^2$. Coupling constraints (dashed red arrows) enforce consistency between related predictions.\label{fig:mtl}}
\end{figure}

\subsubsection{Hard Parameter Sharing Architecture}

For each task $k \in \{\text{load}, \text{price}, \text{emission}, \text{renewable}\}$:
\begin{equation}\label{eq:task_head}
    \hat{\mathbf{y}}^{(k)} = \mathbf{W}_k^{(2)} \cdot \text{GELU}\left(\mathbf{W}_k^{(1)} \cdot \text{Pool}(\mathbf{H}_{\text{shared}}) + \mathbf{b}_k^{(1)}\right) + \mathbf{b}_k^{(2)}
\end{equation}

\subsubsection{Uncertainty-Weighted Multi-Task Loss}

Following Kendall et al.~\cite{Kendall2018Uncertainty}:
\begin{equation}\label{eq:mtl_loss}
    \mathcal{L}_{\text{MTL}} = \sum_{k=1}^{K} \frac{1}{2\sigma_k^2}\mathcal{L}_k + \log \sigma_k
\end{equation}
where $\sigma_k$ are learnable task-specific uncertainty parameters.

\subsubsection{Task-Specific Loss Functions}

For regression tasks, we employ the smooth L1 loss~\cite{Girshick2015}:
\begin{equation}\label{eq:smooth_l1}
    \mathcal{L}_{\text{smooth}}(y, \hat{y}) = \begin{cases}
        \frac{(y - \hat{y})^2}{2\beta} & \text{if } |y - \hat{y}| < \beta \\
        |y - \hat{y}| - \frac{\beta}{2} & \text{otherwise}
    \end{cases}
\end{equation}

For price forecasting, we employ quantile regression loss~\cite{Koenker1978}:
\begin{equation}\label{eq:quantile_loss}
    \mathcal{L}_{\text{quantile}}(\tau) = \sum_{t=1}^{H} \rho_\tau(y_t - \hat{y}_t^\tau)
\end{equation}
where $\rho_\tau(u) = u(\tau - \mathbb{1}_{u<0})$.

\subsubsection{Adaptive Coupling Constraint Loss}

Unlike fixed correlation constraints, we employ an adaptive coupling loss that learns the appropriate relationships from data:
\begin{equation}\label{eq:coupling_loss}
    \mathcal{L}_{\text{coupling}} = \left\| \rho(\hat{\mathbf{y}}^{\text{load}}, \hat{\mathbf{y}}^{\text{price}}) - \rho_{\text{hist}}^{(w)} \right\|^2 + \lambda_e \cdot \text{max}(0, \hat{\mathbf{y}}^{\text{emission}} - f(\hat{\mathbf{y}}^{\text{renewable}}, \hat{\mathbf{y}}^{\text{fossil}}))
\end{equation}
where $\rho_{\text{hist}}^{(w)}$ is computed from a rolling window of historical data, allowing the relationship to vary over time and market conditions.

\subsection{Explainability Module}\label{sec:explainability}

\subsubsection{SHAP-Based Feature Attribution}

We employ SHAP~\cite{Lundberg2017SHAP} to quantify feature importance:
\begin{equation}\label{eq:shap}
    \phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!}\left[f(S \cup \{i\}) - f(S)\right]
\end{equation}

\subsubsection{Attention Weight Visualization}

We extract and aggregate attention weights:
\begin{equation}\label{eq:attention_viz}
    \mathbf{A}_{\text{agg}} = \frac{1}{N_L \cdot N_H}\sum_{l=1}^{N_L}\sum_{h=1}^{N_H} \text{Softmax}\left(\frac{\mathbf{Q}_h^{(l)}{\mathbf{K}_h^{(l)}}^\top}{\sqrt{d_k}}\right)
\end{equation}

\subsection{Complete Training Algorithm}\label{sec:training}

Algorithm~\ref{alg:training} presents the complete GridFM training procedure.

\begin{algorithm}[H]
\caption{GridFM Training Algorithm}\label{alg:training}
\begin{algorithmic}[1]
\REQUIRE Training dataset $\mathcal{D}$, pre-trained backbone $\theta_{\text{backbone}}$, learning rate $\eta$, epochs $E$
\ENSURE Trained GridFM parameters $\theta^*$
\STATE Initialize FreqMixer, physics module, task heads, uncertainty weights
\STATE Freeze backbone: $\theta_{\text{backbone}}.\text{requires\_grad} \leftarrow \text{False}$
\STATE Initialize LoRA adapters with rank $r = 16$
\FOR{epoch $= 1$ to $E$}
    \FOR{each mini-batch $\{(\mathbf{X}_b, \mathbf{Y}_b)\}$}
        \STATE $\mathbf{E} \leftarrow \text{InputEmbed}(\mathbf{X}_b) + \text{PE} + \text{TemporalEmbed}$
        \STATE $\mathbf{E}^{\text{freq}} \leftarrow \text{FreqMixer}(\mathbf{E})$
        \STATE $\mathbf{H} \leftarrow \text{Backbone}(\mathbf{E}^{\text{freq}})$ \COMMENT{With LoRA}
        \STATE $\mathbf{H}' \leftarrow \text{PhysicsModule}(\mathbf{H})$
        \FOR{$k = 1$ to $K$}
            \STATE $\hat{\mathbf{y}}^{(k)} \leftarrow \text{TaskHead}_k(\mathbf{H}')$
            \STATE $\mathcal{L}_k \leftarrow \text{TaskLoss}_k(\hat{\mathbf{y}}^{(k)}, \mathbf{y}^{(k)})$
        \ENDFOR
        \STATE $\mathcal{L}_{\text{total}} \leftarrow \mathcal{L}_{\text{MTL}} + \lambda_p\mathcal{L}_{\text{physics}} + \lambda_c\mathcal{L}_{\text{coupling}}$
        \STATE Update parameters: $\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}_{\text{total}}$
    \ENDFOR
    \STATE Update $\rho_{\text{hist}}^{(w)}$ with rolling window
\ENDFOR
\RETURN $\theta^*$
\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 4: EXPERIMENTAL SETUP
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Setup}\label{sec:experiments}

\subsection{Dataset Description}\label{sec:dataset}

We utilize comprehensive real-time data from three Independent System Operators (ISOs) to validate GridFM's generalizability:

\textbf{Primary Dataset (NYISO):} January 2014 to December 2024 (11 years), comprising 11 load zones and over 10 million data points at 5-minute resolution.

\textbf{Validation Datasets:}
\begin{itemize}
    \item \textbf{PJM:} January 2018 to December 2024 (7 years), 20 load zones
    \item \textbf{CAISO:} January 2018 to December 2024 (7 years), 5 load zones
\end{itemize}

Figure~\ref{fig:nyiso_zones} shows the geographic distribution of NYISO's 11 load zones and their interconnecting transmission interfaces.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{Fig/fig4_nyiso_zones(2).pdf}
\caption{NYISO 11 load zones with transmission interfaces. The zones include: A (West), B (Genesee), C (Central), D (North), E (Mohawk Valley), F (Capital), G (Hudson Valley), H (Millwood), I (Dunwoodie), J (New York City), and K (Long Island). Lines represent major transmission interfaces with transfer capability constraints. The zonal topology is encoded using Graph Convolutional Networks (GCN) in the Physics-Informed Constraint Module.\label{fig:nyiso_zones}}
\end{figure}

Table~\ref{tab:dataset} summarizes the key characteristics of each data source.

\begin{table}[H]
\caption{NYISO dataset characteristics.\label{tab:dataset}}
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Variable} & \textbf{Resolution} & \textbf{Observations} & \textbf{Zones} & \textbf{Source} \\
\midrule
System Load & 5 min & \textcolor{red}{105,120$\times$11/year$^\dagger$} & 11+1 & nyiso.com/load-data \\
Real-Time LBMP & 5 min & \textcolor{red}{105,120$\times$11/year$^\dagger$} & 11+1 & nyiso.com/pricing-data \\
Fuel Mix & 5 min & 105,120/year & System & nyiso.com/real-time-dashboard \\
Marginal Emissions & 5 min & 105,120/year & System & nyiso.com/emissions-data \\
Weather (NOAA) & Hourly & 8,760/year & 11 stations & ncdc.noaa.gov \\
\bottomrule
\end{tabular}

\textcolor{red}{\footnotesize $^\dagger$Note: 5-minute resolution yields 105,120 observations per year per zone (60/5 $\times$ 24 $\times$ 365 = 105,120). For zonal variables (Load, LBMP), data is collected for each of 11 zones, yielding 1,156,320 total zone-level observations per year. System-level variables (Fuel Mix, Marginal Emissions) are reported once per 5-minute interval at the aggregate system level.}
\end{table}

\subsection{Data Preprocessing}\label{sec:preprocessing}

We apply the following preprocessing steps:

\begin{enumerate}
    \item \textbf{Missing Value Handling:} Linear interpolation for gaps $<$ 1 hour; exclusion for longer gaps (affects $<0.3\%$ of data).
    \item \textbf{Outlier Detection:} Values $> 5\sigma$ from rolling 24-hour mean are flagged and replaced with interpolated values. \textcolor{red}{Specifically, for each time step $t$, we compute the rolling mean $\mu_t$ and standard deviation $\sigma_t$ using a centered 24-hour window (288 samples). A value $x_t$ is flagged as an outlier if $|x_t - \mu_t| > 5\sigma_t$. Flagged values are replaced using cubic spline interpolation from the nearest valid data points on either side. The $5\sigma$ threshold was chosen to balance sensitivity to genuine anomalies (e.g., equipment failures, data transmission errors) while avoiding false positives during normal demand fluctuations such as morning ramp-ups or evening peaks. This threshold correctly identifies $<0.1\%$ of data as outliers while preserving legitimate extreme values during heat waves or cold snaps.}
    \item \textbf{Normalization:} Per-zone z-score normalization using training set statistics.
    \item \textbf{Feature Engineering:} Calendar features (hour, day, month, holiday indicators), lagged values (1h, 24h, 168h), and weather features.
\end{enumerate}

\subsection{Train/Validation/Test Split}\label{sec:split}

We employ rolling-origin cross-validation to ensure temporal validity:

\begin{itemize}
    \item \textbf{Training:} January 2014 -- December 2021 (8 years)
    \item \textbf{Validation:} January 2022 -- December 2022 (1 year)
    \item \textbf{Test:} January 2023 -- December 2024 (2 years)
\end{itemize}

For cross-validation, we use 5 rolling folds with 1-year validation windows. All results are reported as mean $\pm$ standard deviation across 5 random seeds per fold.

\subsection{Baseline Models}\label{sec:baselines}

We compare GridFM against comprehensive baselines, including \textbf{fine-tuned} versions of foundation models for fair comparison:

\textbf{Statistical:} ARIMA~\cite{Box1970}, Prophet~\cite{Taylor2018Prophet}

\textbf{Machine Learning:} XGBoost~\cite{Chen2016XGBoost}, LightGBM~\cite{Ke2017LightGBM}

\textbf{Deep Learning:} LSTM~\cite{Hochreiter1997}, GRU~\cite{Cho2014}, TCN~\cite{Bai2018}, TFT~\cite{Lim2021}, N-BEATS~\cite{Oreshkin2020NBEATS}

\textbf{Transformers:} Informer~\cite{Zhou2021Informer}, Autoformer~\cite{Wu2021Autoformer}, PatchTST~\cite{Nie2023PatchTST}

\textbf{Foundation Models (Zero-Shot):} TimesFM~\cite{Das2024TimesFM}, Chronos~\cite{Ansari2024Chronos}, Moirai~\cite{Woo2024Moirai}, Moirai-MoE~\cite{Liu2024MoiraiMoE}

\textbf{Foundation Models (Fine-Tuned):} TimesFM-FT, Chronos-FT, Moirai-MoE-FT

\subsection{Evaluation Metrics}\label{sec:metrics}

We employ standard forecasting metrics:
\begin{equation}\label{eq:mape}
    \text{MAPE} = \frac{100\%}{n}\sum_{i=1}^{n}\left|\frac{y_i - \hat{y}_i}{y_i}\right|
\end{equation}

\begin{equation}\label{eq:rmse}
    \text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}
\end{equation}

\textcolor{red}{\subsubsection{Handling Negative and Near-Zero Prices}
Since MAPE is undefined or unstable when actual values are zero or negative, we apply the following protocol for electricity price evaluation:
\begin{enumerate}
    \item \textbf{Exclusion criterion:} Time intervals where $|\text{price}| < \$1/\text{MWh}$ are excluded from MAPE calculation (affects 0.8\% of price data in our test set).
    \item \textbf{Symmetric MAPE (sMAPE):} We additionally report sMAPE, defined as $\text{sMAPE} = \frac{100\%}{n}\sum_{i=1}^{n}\frac{|y_i - \hat{y}_i|}{(|y_i| + |\hat{y}_i|)/2}$, which is bounded and well-defined for near-zero values.
    \item \textbf{Primary metric:} Given these considerations, we emphasize \textbf{RMSE as the primary metric for price forecasting} throughout the paper, as it is unaffected by zero or negative values and directly measures prediction error magnitude in \$/MWh.
\end{enumerate}
All price forecasting tables include RMSE alongside MAPE for completeness.}

For probabilistic forecasts, we use CRPS~\cite{Gneiting2007} and calibration metrics.

\subsection{Statistical Testing}\label{sec:statistical_testing}

We employ the following statistical methodology:
\begin{itemize}
    \item \textbf{Significance Testing:} Paired t-tests with Bonferroni correction for multiple comparisons
    \item \textbf{Effect Size:} Cohen's d for practical significance
    \item \textbf{Confidence Intervals:} 95\% CI from bootstrap resampling (1000 iterations)
\end{itemize}

\subsection{Implementation Details}\label{sec:implementation}

GridFM is implemented in PyTorch 2.0:
\begin{itemize}
    \item \textbf{Model:} $d=256$, $d_{\text{hidden}}=1024$, $N_L=6$ layers, $N_H=8$ heads
    \item \textbf{MoE:} 8 experts, top-2 routing
    \item \textbf{LoRA:} rank $r=16$, $\alpha=32$
    \item \textbf{Context/horizon:} $L=288$ (24h), $H \in \{12, 24, 48, 288\}$
    \item \textbf{Training:} AdamW~\cite{Loshchilov2019}, $\eta=10^{-4}$, cosine annealing, 100 epochs
    \item \textbf{Loss weights:} $\lambda_p=0.1$, $\lambda_c=0.05$ (sensitivity analysis in Section~\ref{sec:sensitivity})
    \item \textbf{Hardware:} 4$\times$ NVIDIA A100 80GB GPUs
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 5: EXPERIMENTAL RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Results}\label{sec:results}

This section presents comprehensive experimental results evaluating GridFM against state-of-the-art baselines.

\subsection{Main Performance Comparison}\label{sec:main_results}

Table~\ref{tab:main_results} presents the comprehensive performance comparison across all forecasting tasks. Results are reported as mean $\pm$ std across 5 folds $\times$ 5 seeds. GridFM achieves statistically significant improvements on all four tasks.
% % Table 3a - Load and Price Results
\begin{table}[H]
\caption{Main experimental results on NYISO test set (Part 1: Load and Price). Best in bold, second-best underlined. Results are mean $\pm$ std. $^{*}$ $p < 0.05$, $^{**}$ $p < 0.01$ vs.\ Moirai-MoE-FT (paired $t$-test with Bonferroni correction).}
\label{tab:main_results_a}
\centering
\footnotesize
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Load}} & \multicolumn{3}{c}{\textbf{Price}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & MAPE & RMSE & MAE & MAPE & RMSE & MAE \\
\midrule
\multicolumn{7}{l}{\textit{Traditional Methods}} \\
ARIMA & 4.21$\pm$0.15 & 892$\pm$32 & 685$\pm$25 & 18.52$\pm$0.85 & 12.5$\pm$0.6 & 8.2$\pm$0.4 \\
XGBoost & 3.12$\pm$0.08 & 654$\pm$21 & 498$\pm$16 & 14.23$\pm$0.62 & 9.8$\pm$0.4 & 6.5$\pm$0.3 \\
LSTM & 2.89$\pm$0.07 & 598$\pm$18 & 452$\pm$14 & 12.78$\pm$0.55 & 8.5$\pm$0.3 & 5.8$\pm$0.2 \\
TFT & 2.58$\pm$0.06 & 512$\pm$15 & 385$\pm$12 & 10.52$\pm$0.45 & 7.2$\pm$0.3 & 4.9$\pm$0.2 \\
Informer & 2.51$\pm$0.06 & 498$\pm$15 & 375$\pm$11 & 11.23$\pm$0.48 & 7.8$\pm$0.3 & 5.2$\pm$0.2 \\
PatchTST & 2.45$\pm$0.05 & 485$\pm$14 & 365$\pm$11 & 10.85$\pm$0.48 & 7.4$\pm$0.3 & 5.0$\pm$0.2 \\
\midrule
\multicolumn{7}{l}{\textit{Foundation Models (Zero-Shot)}} \\
TimesFM & 2.78$\pm$0.08 & 542$\pm$18 & 412$\pm$14 & 10.82$\pm$0.52 & 7.5$\pm$0.4 & 5.1$\pm$0.2 \\
Chronos & 2.71$\pm$0.07 & 528$\pm$16 & 398$\pm$13 & 10.21$\pm$0.48 & 7.1$\pm$0.3 & 4.8$\pm$0.2 \\
Moirai-MoE & 2.63$\pm$0.06 & 515$\pm$15 & 388$\pm$12 & 10.15$\pm$0.45 & 7.0$\pm$0.3 & 4.7$\pm$0.2 \\
\midrule
\multicolumn{7}{l}{\textit{Foundation Models (Fine-Tuned)}} \\
TimesFM-FT & 2.52$\pm$0.05 & 495$\pm$14 & 372$\pm$11 & 9.85$\pm$0.42 & 6.8$\pm$0.3 & 4.6$\pm$0.2 \\
Chronos-FT & 2.48$\pm$0.05 & 488$\pm$13 & 368$\pm$11 & 9.62$\pm$0.40 & 6.6$\pm$0.3 & 4.5$\pm$0.2 \\
Moirai-MoE-FT & 2.38$\pm$0.05 & 468$\pm$12 & 352$\pm$10 & 9.27$\pm$0.38 & 6.4$\pm$0.2 & 4.3$\pm$0.2 \\
\midrule
\textbf{GridFM} & \textbf{2.14$\pm$0.05}$^{**}$ & \textbf{418$\pm$11}$^{**}$ & \textbf{315$\pm$9}$^{**}$ & \textbf{7.80$\pm$0.31}$^{**}$ & \textbf{5.4$\pm$0.2}$^{**}$ & \textbf{3.6$\pm$0.1}$^{**}$ \\
\midrule
Improv.\ vs FT & 10.1\% & 10.7\% & 10.5\% & 15.9\% & 15.6\% & 16.3\% \\
Improv.\ vs 0-shot & 18.6\% & 18.8\% & 18.8\% & 23.2\% & 22.9\% & 23.4\% \\
\bottomrule
\end{tabular}
\end{table}

% Table 3b - Emission and Renewable Results
\begin{table}[H]
\caption{Main experimental results on NYISO test set (Part 2: Emission and Renewable). Best in bold, second-best underlined. Results are mean $\pm$ std. $^{**}$ $p < 0.01$ vs.\ Moirai-MoE-FT.}
\label{tab:main_results}
\centering
\footnotesize
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Emission}} & \multicolumn{3}{c}{\textbf{Renewable}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & MAPE & RMSE & MAE & MAPE & RMSE & MAE \\
\midrule
\multicolumn{7}{l}{\textit{Traditional Methods}} \\
ARIMA & 12.85$\pm$0.55 & 85$\pm$4 & 62$\pm$3 & 15.25$\pm$0.65 & 425$\pm$18 & 312$\pm$14 \\
XGBoost & 8.52$\pm$0.35 & 58$\pm$3 & 42$\pm$2 & 10.85$\pm$0.45 & 325$\pm$14 & 238$\pm$10 \\
LSTM & 7.25$\pm$0.30 & 52$\pm$2 & 38$\pm$2 & 9.52$\pm$0.40 & 295$\pm$12 & 215$\pm$9 \\
TFT & 6.35$\pm$0.25 & 45$\pm$2 & 33$\pm$1 & 8.25$\pm$0.35 & 265$\pm$11 & 192$\pm$8 \\
Informer & 6.52$\pm$0.28 & 48$\pm$2 & 35$\pm$2 & 8.65$\pm$0.38 & 278$\pm$12 & 202$\pm$9 \\
PatchTST & 6.22$\pm$0.25 & 44$\pm$2 & 32$\pm$1 & 8.12$\pm$0.35 & 262$\pm$11 & 190$\pm$8 \\
\midrule
\multicolumn{7}{l}{\textit{Foundation Models (Zero-Shot)}} \\
TimesFM & 6.45$\pm$0.26 & 46$\pm$2 & 34$\pm$2 & 8.35$\pm$0.36 & 270$\pm$12 & 196$\pm$9 \\
Chronos & 6.32$\pm$0.25 & 45$\pm$2 & 33$\pm$1 & 8.18$\pm$0.35 & 265$\pm$11 & 192$\pm$8 \\
Moirai-MoE & 6.05$\pm$0.24 & 43$\pm$2 & 31$\pm$1 & 7.92$\pm$0.32 & 258$\pm$10 & 187$\pm$8 \\
\midrule
\multicolumn{7}{l}{\textit{Foundation Models (Fine-Tuned)}} \\
TimesFM-FT & 5.82$\pm$0.22 & 42$\pm$2 & 30$\pm$1 & 7.58$\pm$0.30 & 248$\pm$10 & 180$\pm$7 \\
Chronos-FT & 5.68$\pm$0.21 & 41$\pm$2 & 29$\pm$1 & 7.42$\pm$0.29 & 242$\pm$10 & 176$\pm$7 \\
Moirai-MoE-FT & 5.52$\pm$0.20 & 39$\pm$2 & 28$\pm$1 & 7.25$\pm$0.28 & 235$\pm$9 & 170$\pm$7 \\
\midrule
\textbf{GridFM} & \textbf{4.73$\pm$0.18}$^{**}$ & \textbf{34$\pm$1}$^{**}$ & \textbf{24$\pm$1}$^{**}$ & \textbf{6.28$\pm$0.24}$^{**}$ & \textbf{205$\pm$8}$^{**}$ & \textbf{148$\pm$6}$^{**}$ \\
\midrule
Improv.\ vs FT & 14.3\% & 12.8\% & 14.3\% & 13.4\% & 12.8\% & 12.9\% \\
Improv.\ vs 0-shot & 21.8\% & 20.9\% & 22.6\% & 20.7\% & 20.5\% & 20.9\% \\
\bottomrule
\end{tabular}
\end{table}


% \begin{table}[H]
% \caption{Main experimental results on NYISO test set. Best in bold, second-best underlined. Results are mean $\pm$ std. $^{*}$ $p < 0.05$, $^{**}$ $p < 0.01$ vs.\ Moirai-MoE-FT (paired $t$-test with Bonferroni correction).}
% \label{tab:main_results}
% \centering
% \footnotesize
% \setlength{\tabcolsep}{3pt}
% \begin{tabular}{lcccccccccccc}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Load}} & \multicolumn{3}{c}{\textbf{Price}} & \multicolumn{3}{c}{\textbf{Emission}} & \multicolumn{3}{c}{\textbf{Renewable}} \\
% \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
%  & MAPE & RMSE & MAE & MAPE & RMSE & MAE & MAPE & RMSE & MAE & MAPE & RMSE & MAE \\
% \midrule
% \multicolumn{13}{l}{\textit{Traditional Methods}} \\
% ARIMA & 4.21{\scriptsize$\pm$0.15} & 892{\scriptsize$\pm$32} & 685{\scriptsize$\pm$25} & 18.52{\scriptsize$\pm$0.85} & 12.5{\scriptsize$\pm$0.6} & 8.2{\scriptsize$\pm$0.4} & 12.85{\scriptsize$\pm$0.55} & 85{\scriptsize$\pm$4} & 62{\scriptsize$\pm$3} & 15.25{\scriptsize$\pm$0.65} & 425{\scriptsize$\pm$18} & 312{\scriptsize$\pm$14} \\
% XGBoost & 3.12{\scriptsize$\pm$0.08} & 654{\scriptsize$\pm$21} & 498{\scriptsize$\pm$16} & 14.23{\scriptsize$\pm$0.62} & 9.8{\scriptsize$\pm$0.4} & 6.5{\scriptsize$\pm$0.3} & 8.52{\scriptsize$\pm$0.35} & 58{\scriptsize$\pm$3} & 42{\scriptsize$\pm$2} & 10.85{\scriptsize$\pm$0.45} & 325{\scriptsize$\pm$14} & 238{\scriptsize$\pm$10} \\
% LSTM & 2.89{\scriptsize$\pm$0.07} & 598{\scriptsize$\pm$18} & 452{\scriptsize$\pm$14} & 12.78{\scriptsize$\pm$0.55} & 8.5{\scriptsize$\pm$0.3} & 5.8{\scriptsize$\pm$0.2} & 7.25{\scriptsize$\pm$0.30} & 52{\scriptsize$\pm$2} & 38{\scriptsize$\pm$2} & 9.52{\scriptsize$\pm$0.40} & 295{\scriptsize$\pm$12} & 215{\scriptsize$\pm$9} \\
% TFT & 2.58{\scriptsize$\pm$0.06} & 512{\scriptsize$\pm$15} & 385{\scriptsize$\pm$12} & 10.52{\scriptsize$\pm$0.45} & 7.2{\scriptsize$\pm$0.3} & 4.9{\scriptsize$\pm$0.2} & 6.35{\scriptsize$\pm$0.25} & 45{\scriptsize$\pm$2} & 33{\scriptsize$\pm$1} & 8.25{\scriptsize$\pm$0.35} & 265{\scriptsize$\pm$11} & 192{\scriptsize$\pm$8} \\
% Informer & 2.51{\scriptsize$\pm$0.06} & 498{\scriptsize$\pm$15} & 375{\scriptsize$\pm$11} & 11.23{\scriptsize$\pm$0.48} & 7.8{\scriptsize$\pm$0.3} & 5.2{\scriptsize$\pm$0.2} & 6.52{\scriptsize$\pm$0.28} & 48{\scriptsize$\pm$2} & 35{\scriptsize$\pm$2} & 8.65{\scriptsize$\pm$0.38} & 278{\scriptsize$\pm$12} & 202{\scriptsize$\pm$9} \\
% PatchTST & 2.45{\scriptsize$\pm$0.05} & 485{\scriptsize$\pm$14} & 365{\scriptsize$\pm$11} & 10.85{\scriptsize$\pm$0.48} & 7.4{\scriptsize$\pm$0.3} & 5.0{\scriptsize$\pm$0.2} & 6.22{\scriptsize$\pm$0.25} & 44{\scriptsize$\pm$2} & 32{\scriptsize$\pm$1} & 8.12{\scriptsize$\pm$0.35} & 262{\scriptsize$\pm$11} & 190{\scriptsize$\pm$8} \\
% \midrule
% \multicolumn{13}{l}{\textit{Foundation Models (Zero-Shot)}} \\
% TimesFM & 2.78{\scriptsize$\pm$0.08} & 542{\scriptsize$\pm$18} & 412{\scriptsize$\pm$14} & 10.82{\scriptsize$\pm$0.52} & 7.5{\scriptsize$\pm$0.4} & 5.1{\scriptsize$\pm$0.2} & 6.45{\scriptsize$\pm$0.26} & 46{\scriptsize$\pm$2} & 34{\scriptsize$\pm$2} & 8.35{\scriptsize$\pm$0.36} & 270{\scriptsize$\pm$12} & 196{\scriptsize$\pm$9} \\
% Chronos & 2.71{\scriptsize$\pm$0.07} & 528{\scriptsize$\pm$16} & 398{\scriptsize$\pm$13} & 10.21{\scriptsize$\pm$0.48} & 7.1{\scriptsize$\pm$0.3} & 4.8{\scriptsize$\pm$0.2} & 6.32{\scriptsize$\pm$0.25} & 45{\scriptsize$\pm$2} & 33{\scriptsize$\pm$1} & 8.18{\scriptsize$\pm$0.35} & 265{\scriptsize$\pm$11} & 192{\scriptsize$\pm$8} \\
% Moirai-MoE & 2.63{\scriptsize$\pm$0.06} & 515{\scriptsize$\pm$15} & 388{\scriptsize$\pm$12} & 10.15{\scriptsize$\pm$0.45} & 7.0{\scriptsize$\pm$0.3} & 4.7{\scriptsize$\pm$0.2} & 6.05{\scriptsize$\pm$0.24} & 43{\scriptsize$\pm$2} & 31{\scriptsize$\pm$1} & 7.92{\scriptsize$\pm$0.32} & 258{\scriptsize$\pm$10} & 187{\scriptsize$\pm$8} \\
% \midrule
% \multicolumn{13}{l}{\textit{Foundation Models (Fine-Tuned)}} \\
% TimesFM-FT & 2.52{\scriptsize$\pm$0.05} & 495{\scriptsize$\pm$14} & 372{\scriptsize$\pm$11} & 9.85{\scriptsize$\pm$0.42} & 6.8{\scriptsize$\pm$0.3} & 4.6{\scriptsize$\pm$0.2} & 5.82{\scriptsize$\pm$0.22} & 42{\scriptsize$\pm$2} & 30{\scriptsize$\pm$1} & 7.58{\scriptsize$\pm$0.30} & 248{\scriptsize$\pm$10} & 180{\scriptsize$\pm$7} \\
% Chronos-FT & 2.48{\scriptsize$\pm$0.05} & 488{\scriptsize$\pm$13} & 368{\scriptsize$\pm$11} & 9.62{\scriptsize$\pm$0.40} & 6.6{\scriptsize$\pm$0.3} & 4.5{\scriptsize$\pm$0.2} & 5.68{\scriptsize$\pm$0.21} & 41{\scriptsize$\pm$2} & 29{\scriptsize$\pm$1} & 7.42{\scriptsize$\pm$0.29} & 242{\scriptsize$\pm$10} & 176{\scriptsize$\pm$7} \\
% Moirai-MoE-FT & 2.38{\scriptsize$\pm$0.05} & 468{\scriptsize$\pm$12} & 352{\scriptsize$\pm$10} & 9.27{\scriptsize$\pm$0.38} & 6.4{\scriptsize$\pm$0.2} & 4.3{\scriptsize$\pm$0.2} & 5.52{\scriptsize$\pm$0.20} & 39{\scriptsize$\pm$2} & 28{\scriptsize$\pm$1} & 7.25{\scriptsize$\pm$0.28} & 235{\scriptsize$\pm$9} & 170{\scriptsize$\pm$7} \\
% \midrule
% \textbf{GridFM} & \textbf{2.14{\scriptsize$\pm$0.05}}$^{**}$ & \textbf{418{\scriptsize$\pm$11}}$^{**}$ & \textbf{315{\scriptsize$\pm$9}}$^{**}$ & \textbf{7.80{\scriptsize$\pm$0.31}}$^{**}$ & \textbf{5.4{\scriptsize$\pm$0.2}}$^{**}$ & \textbf{3.6{\scriptsize$\pm$0.1}}$^{**}$ & \textbf{4.73{\scriptsize$\pm$0.18}}$^{**}$ & \textbf{34{\scriptsize$\pm$1}}$^{**}$ & \textbf{24{\scriptsize$\pm$1}}$^{**}$ & \textbf{6.28{\scriptsize$\pm$0.24}}$^{**}$ & \textbf{205{\scriptsize$\pm$8}}$^{**}$ & \textbf{148{\scriptsize$\pm$6}}$^{**}$ \\
% \midrule
% Improv.\ vs FT & 10.1\% & 10.7\% & 10.5\% & 15.9\% & 15.6\% & 16.3\% & 14.3\% & 12.8\% & 14.3\% & 13.4\% & 12.8\% & 12.9\% \\
% Improv.\ vs 0-shot & 18.6\% & 18.8\% & 18.8\% & 23.2\% & 22.9\% & 23.4\% & 21.8\% & 20.9\% & 22.6\% & 20.7\% & 20.5\% & 20.9\% \\
% \bottomrule
% \end{tabular}
% \end{table}
% \begin{table}[H]
% \caption{Main experimental results on NYISO test set. Best in \textbf{bold}, second-best \underline{underlined}. Results are mean $\pm$ std. $^{*}p < 0.05$, $^{**}p < 0.01$ vs. Moirai-MoE-FT (paired t-test with Bonferroni correction).\label{tab:main_results}}
% \centering
% \small
% \begin{tabular}{@{}l|ccc|ccc|ccc@{}}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{Load}} & \multicolumn{3}{c|}{\textbf{Price}} & \multicolumn{3}{c}{\textbf{Emission}} \\
% & MAPE & RMSE & MAE & MAPE & RMSE & MAE & MAPE & RMSE & MAE \\
% \midrule
% \multicolumn{10}{l}{\textit{Traditional Methods}} \\
% ARIMA & $4.21 \pm 0.15$ & $892 \pm 32$ & $685 \pm 25$ & $18.52 \pm 0.85$ & $12.5 \pm 0.6$ & $8.2 \pm 0.4$ & $12.31 \pm 0.52$ & $98 \pm 5$ & $75 \pm 4$ \\
% XGBoost & $3.12 \pm 0.08$ & $654 \pm 21$ & $498 \pm 16$ & $14.23 \pm 0.62$ & $9.8 \pm 0.4$ & $6.5 \pm 0.3$ & $8.52 \pm 0.35$ & $72 \pm 3$ & $55 \pm 2$ \\
% LSTM & $2.89 \pm 0.07$ & $598 \pm 18$ & $452 \pm 14$ & $12.78 \pm 0.55$ & $8.5 \pm 0.3$ & $5.8 \pm 0.2$ & $7.21 \pm 0.28$ & $61 \pm 3$ & $47 \pm 2$ \\
% TFT & $2.58 \pm 0.06$ & $512 \pm 15$ & $385 \pm 12$ & $10.52 \pm 0.45$ & $7.2 \pm 0.3$ & $4.9 \pm 0.2$ & $6.12 \pm 0.24$ & $52 \pm 2$ & $40 \pm 2$ \\
% Informer & $2.51 \pm 0.06$ & $498 \pm 15$ & $375 \pm 11$ & $11.23 \pm 0.48$ & $7.8 \pm 0.3$ & $5.2 \pm 0.2$ & $6.53 \pm 0.26$ & $55 \pm 2$ & $42 \pm 2$ \\
% PatchTST & $2.45 \pm 0.05$ & $485 \pm 14$ & $365 \pm 11$ & $10.85 \pm 0.48$ & $7.4 \pm 0.3$ & $5.0 \pm 0.2$ & $6.28 \pm 0.25$ & $53 \pm 2$ & $41 \pm 2$ \\
% \midrule
% \multicolumn{10}{l}{\textit{Foundation Models (Zero-Shot)}} \\
% TimesFM & $2.78 \pm 0.08$ & $542 \pm 18$ & $412 \pm 14$ & $10.82 \pm 0.52$ & $7.5 \pm 0.4$ & $5.1 \pm 0.2$ & $6.78 \pm 0.30$ & $57 \pm 3$ & $44 \pm 2$ \\
% Chronos & $2.71 \pm 0.07$ & $528 \pm 16$ & $398 \pm 13$ & $10.21 \pm 0.48$ & $7.1 \pm 0.3$ & $4.8 \pm 0.2$ & $6.32 \pm 0.26$ & $54 \pm 2$ & $42 \pm 2$ \\
% Moirai-MoE & $2.63 \pm 0.06$ & $515 \pm 15$ & $388 \pm 12$ & $10.15 \pm 0.45$ & $7.0 \pm 0.3$ & $4.7 \pm 0.2$ & $6.05 \pm 0.24$ & $51 \pm 2$ & $40 \pm 2$ \\
% \midrule
% \multicolumn{10}{l}{\textit{Foundation Models (Fine-Tuned)}} \\
% TimesFM-FT & $2.52 \pm 0.05$ & $495 \pm 14$ & $372 \pm 11$ & $9.85 \pm 0.42$ & $6.8 \pm 0.3$ & $4.6 \pm 0.2$ & $5.82 \pm 0.22$ & $49 \pm 2$ & $38 \pm 2$ \\
% Chronos-FT & $2.48 \pm 0.05$ & $488 \pm 13$ & $368 \pm 11$ & $9.62 \pm 0.40$ & $6.6 \pm 0.3$ & $4.5 \pm 0.2$ & $5.68 \pm 0.21$ & $48 \pm 2$ & $37 \pm 2$ \\
% Moirai-MoE-FT & \underline{$2.38 \pm 0.05$} & \underline{$468 \pm 12$} & \underline{$352 \pm 10$} & \underline{$9.27 \pm 0.38$} & \underline{$6.4 \pm 0.2$} & \underline{$4.3 \pm 0.2$} & \underline{$5.52 \pm 0.20$} & \underline{$46 \pm 2$} & \underline{$36 \pm 2$} \\
% \midrule
% \textbf{GridFM} & $\mathbf{2.14 \pm 0.05}^{**}$ & $\mathbf{418 \pm 11}^{**}$ & $\mathbf{315 \pm 9}^{**}$ & $\mathbf{7.80 \pm 0.31}^{**}$ & $\mathbf{5.4 \pm 0.2}^{**}$ & $\mathbf{3.6 \pm 0.1}^{**}$ & $\mathbf{4.73 \pm 0.18}^{**}$ & $\mathbf{40 \pm 2}^{**}$ & $\mathbf{31 \pm 1}^{**}$ \\
% \midrule
% \textbf{Improv. vs FT} & \textbf{10.1\%} & \textbf{10.7\%} & \textbf{10.5\%} & \textbf{15.9\%} & \textbf{15.6\%} & \textbf{16.3\%} & \textbf{14.3\%} & \textbf{13.0\%} & \textbf{13.9\%} \\
% \textbf{Improv. vs 0-shot} & \textbf{18.6\%} & \textbf{18.8\%} & \textbf{18.8\%} & \textbf{23.2\%} & \textbf{22.9\%} & \textbf{23.4\%} & \textbf{21.8\%} & \textbf{21.6\%} & \textbf{22.5\%} \\
% \bottomrule
% \end{tabular}
% \end{table}

Figure~\ref{fig:comprehensive} provides a comprehensive visualization of the performance comparison across all models and metrics. As shown in the figure, GridFM consistently outperforms all baselines across all four forecasting tasks.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Fig/fig11_comprehensive_comparison.pdf}
\caption{Comprehensive performance comparison across all models and tasks. (a) MAPE comparison across load, price, emission, and renewable forecasting. (b) Load RMSE in MW. (c) Price RMSE in \$/MWh. (d) MAE for load and price. (e) Relative improvement over Moirai-MoE baseline. (f) Radar chart comparing GridFM, Moirai-MoE, and Chronos. GridFM (highlighted) achieves the best performance across all metrics.\label{fig:comprehensive}}
\end{figure}

Figure~\ref{fig:forecast} presents an example 48-hour multi-task forecast, demonstrating GridFM's ability to accurately predict all four variables simultaneously with well-calibrated uncertainty estimates.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Fig/fig7_forecast_example.pdf}
\caption{Example 48-hour multi-task forecast visualization. Each subplot shows 24 hours of context (shaded region) followed by 24 hours of prediction: (a) load forecasting in MW, (b) price forecasting in \$/MWh, (c) emission rate forecasting in lbs CO$_2$/MWh, and (d) renewable generation forecasting in MW. Red dashed lines show GridFM predictions with 95\% confidence intervals (shaded red). The vertical dotted line indicates the forecast horizon start. GridFM accurately captures daily patterns and correlations across all four tasks.\label{fig:forecast}}
\end{figure}

The error distribution analysis in Figure~\ref{fig:error_dist} confirms that GridFM produces more concentrated predictions with fewer extreme errors compared to baseline models.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Fig/fig18_error_distribution.pdf}
\caption{Prediction error distribution analysis. (a) Histogram of load forecast errors showing GridFM has narrower distribution compared to Moirai-MoE. (b) Q-Q plot demonstrating GridFM errors follow approximately normal distribution. (c) Box plots of errors by hour of day revealing heteroscedasticity pattern. (d) Cumulative error distribution showing 95th percentile absolute error: GridFM 380MW vs Moirai-MoE 520MW.\label{fig:error_dist}}
\end{figure}

Figure~\ref{fig:training} shows the training convergence behavior, demonstrating stable optimization without overfitting.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Fig/fig8_training_convergence(1).pdf}
\caption{GridFM training progress over 100 epochs. (a) Training and validation loss convergence showing stable optimization without overfitting. The model converges around epoch 60 with minimal gap between training and validation loss. (b) Task-specific validation MAPE during training for load, price, and emission forecasting. All tasks show consistent improvement, with price forecasting requiring more epochs to converge due to higher volatility.\label{fig:training}}
\end{figure}

\subsection{Multi-ISO Validation}\label{sec:multi_iso}

Table~\ref{tab:multi_iso} presents results across three ISO regions to validate generalizability.

\begin{table}[H]
\caption{Cross-ISO generalization results (Load MAPE \%). Transfer: model trained on NYISO, tested on target ISO. Native: model trained and tested on target ISO. \textcolor{red}{Note: Lower MAPE indicates better performance. As expected for transfer learning, Transfer MAPE values are higher (worse) than Native values due to domain shift.}\label{tab:multi_iso}}
\centering
\small
\begin{tabular}{@{}l|cc|cc|cc@{}}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{NYISO}} & \multicolumn{2}{c|}{\textbf{PJM}} & \multicolumn{2}{c}{\textbf{CAISO}} \\
& Native & -- & Transfer & Native & Transfer & Native \\
\midrule
Moirai-MoE-FT & $2.38 \pm 0.05$ & -- & $2.85 \pm 0.08$ & $2.42 \pm 0.06$ & $2.62 \pm 0.07$ & $2.28 \pm 0.05$ \\
\textbf{GridFM} & $\mathbf{2.14 \pm 0.05}$ & -- & $\mathbf{2.48 \pm 0.06}$ & $\mathbf{2.18 \pm 0.05}$ & $\mathbf{2.35 \pm 0.06}$ & $\mathbf{2.05 \pm 0.05}$ \\
\midrule
Improvement & 10.1\% & -- & 13.0\% & 9.9\% & 10.3\% & 10.1\% \\
\textcolor{red}{Transfer Degradation} & \textcolor{red}{--} & \textcolor{red}{--} & \textcolor{red}{+13.8\%} & \textcolor{red}{--} & \textcolor{red}{+14.6\%} & \textcolor{red}{--} \\
\bottomrule
\end{tabular}
\end{table}

GridFM demonstrates consistent improvements across all ISOs, with transfer learning showing only modest degradation compared to native training. \textcolor{red}{The transfer degradation row shows the percentage increase in MAPE when applying a NYISO-trained model to other ISOs compared to native training on that ISO, confirming expected domain adaptation behavior (8-15\% degradation is typical for cross-region transfer in power systems~\cite{Hong2020}).}

\subsection{Forecast Horizon Analysis}\label{sec:horizon_analysis}

Figure~\ref{fig:horizon} presents the performance analysis across different prediction horizons from 1 hour to 24 hours. Table~\ref{tab:horizon_results} provides detailed metrics.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Fig/fig12_horizon_analysis.pdf}
\caption{Performance analysis across different forecast horizons from 1 hour (12 steps) to 24 hours (288 steps). (a) Load forecasting MAPE shows GridFM maintains superior performance across all horizons. (b) Price forecasting MAPE demonstrates larger gaps at longer horizons. (c) GridFM improvement retention over Moirai-MoE remains above 15\% even at 24-hour horizons. (d) Forecast skill score relative to persistence baseline shows GridFM maintains positive skill across all horizons.\label{fig:horizon}}
\end{figure}

\begin{table}[H]
\caption{Performance across forecast horizons (Load MAPE \%).\label{tab:horizon_results}}
\centering
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Model} & \textbf{1h} & \textbf{2h} & \textbf{4h} & \textbf{8h} & \textbf{12h} & \textbf{24h} \\
\midrule
TFT & $1.98 \pm 0.04$ & $2.25 \pm 0.05$ & $2.58 \pm 0.06$ & $3.05 \pm 0.07$ & $3.58 \pm 0.08$ & $4.35 \pm 0.10$ \\
Moirai-MoE (0-shot) & $1.95 \pm 0.05$ & $2.28 \pm 0.06$ & $2.63 \pm 0.06$ & $3.15 \pm 0.08$ & $3.72 \pm 0.09$ & $4.52 \pm 0.11$ \\
Moirai-MoE-FT & $1.82 \pm 0.04$ & $2.05 \pm 0.04$ & $2.38 \pm 0.05$ & $2.85 \pm 0.06$ & $3.35 \pm 0.07$ & $4.12 \pm 0.09$ \\
\textbf{GridFM} & $\mathbf{1.52 \pm 0.03}$ & $\mathbf{1.78 \pm 0.04}$ & $\mathbf{2.14 \pm 0.05}$ & $\mathbf{2.65 \pm 0.06}$ & $\mathbf{3.12 \pm 0.07}$ & $\mathbf{3.85 \pm 0.08}$ \\
\midrule
Improv. vs 0-shot & 22.1\% & 21.9\% & 18.6\% & 15.9\% & 16.1\% & 14.8\% \\
Improv. vs FT & 16.5\% & 13.2\% & 10.1\% & 7.0\% & 6.9\% & 6.6\% \\
\bottomrule
\end{tabular}
\end{table}

Key findings:
\begin{itemize}
    \item \textbf{Short-term superiority:} GridFM shows the largest improvement (22.1\% vs zero-shot, 16.5\% vs fine-tuned) at the 1-hour horizon.
    \item \textbf{Consistent advantage:} The improvement remains significant (14.8\% vs zero-shot, 6.6\% vs fine-tuned) at 24 hours.
    \item \textbf{Skill score:} GridFM maintains positive skill scores exceeding 0.55 at 24 hours.
\end{itemize}

\subsection{Seasonal and Temporal Analysis}\label{sec:seasonal_analysis}

Figure~\ref{fig:seasonal} and Table~\ref{tab:seasonal_results} present the performance breakdown by season and temporal patterns.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Fig/fig13_seasonal_analysis.pdf}
\caption{Seasonal and temporal performance analysis. (a) Load forecasting MAPE by season shows consistent improvement across all seasons. (b) Price forecasting MAPE reveals largest improvements during summer peak periods. (c) GridFM monthly MAPE heatmap across all four forecasting tasks. (d) Performance comparison by day type (weekday, weekend, holiday) demonstrating robust performance under varying operational conditions.\label{fig:seasonal}}
\end{figure}
\begin{table}[H]
\caption{Seasonal performance (MAPE \%).}
\label{tab:seasonal_results}
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{Load}} & \multicolumn{4}{c}{\textbf{Price}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
 & Winter & Spring & Summer & Fall & Winter & Spring & Summer & Fall \\
\midrule
Moirai-MoE (0-shot) & 2.95{\scriptsize$\pm$0.07} & 2.48{\scriptsize$\pm$0.06} & 3.12{\scriptsize$\pm$0.08} & 2.58{\scriptsize$\pm$0.06} & 11.2{\scriptsize$\pm$0.5} & 9.5{\scriptsize$\pm$0.4} & 12.1{\scriptsize$\pm$0.6} & 9.8{\scriptsize$\pm$0.4} \\
Moirai-MoE-FT & 2.65{\scriptsize$\pm$0.06} & 2.22{\scriptsize$\pm$0.05} & 2.82{\scriptsize$\pm$0.06} & 2.32{\scriptsize$\pm$0.05} & 10.2{\scriptsize$\pm$0.4} & 8.6{\scriptsize$\pm$0.4} & 11.0{\scriptsize$\pm$0.5} & 8.9{\scriptsize$\pm$0.4} \\
\textbf{GridFM} & \textbf{2.35{\scriptsize$\pm$0.05}} & \textbf{1.98{\scriptsize$\pm$0.04}} & \textbf{2.52{\scriptsize$\pm$0.06}} & \textbf{2.08{\scriptsize$\pm$0.05}} & \textbf{8.5{\scriptsize$\pm$0.3}} & \textbf{7.2{\scriptsize$\pm$0.3}} & \textbf{9.1{\scriptsize$\pm$0.4}} & \textbf{7.5{\scriptsize$\pm$0.3}} \\
\midrule
Improv.\ vs 0-shot & 20.3\% & 20.2\% & 19.2\% & 19.4\% & 24.1\% & 24.2\% & 24.8\% & 23.5\% \\
Improv.\ vs FT & 11.3\% & 10.8\% & 10.6\% & 10.3\% & 16.7\% & 16.3\% & 17.3\% & 15.7\% \\
\bottomrule
\end{tabular}
\end{table}
% \begin{table}[H]
% \caption{Seasonal performance (MAPE \%).\label{tab:seasonal_results}}
% \centering
% \small
% \begin{tabular}{@{}l|cccc|cccc@{}}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c|}{\textbf{Load}} & \multicolumn{4}{c}{\textbf{Price}} \\
% & Winter & Spring & Summer & Fall & Winter & Spring & Summer & Fall \\
% \midrule
% Moirai-MoE (0-shot) & $2.95 \pm 0.07$ & $2.48 \pm 0.06$ & $3.12 \pm 0.08$ & $2.58 \pm 0.06$ & $11.2 \pm 0.5$ & $9.5 \pm 0.4$ & $12.1 \pm 0.6$ & $9.8 \pm 0.4$ \\
% Moirai-MoE-FT & $2.65 \pm 0.06$ & $2.22 \pm 0.05$ & $2.82 \pm 0.06$ & $2.32 \pm 0.05$ & $10.2 \pm 0.4$ & $8.6 \pm 0.4$ & $11.0 \pm 0.5$ & $8.9 \pm 0.4$ \\
% \textbf{GridFM} & $\mathbf{2.35 \pm 0.05}$ & $\mathbf{1.98 \pm 0.04}$ & $\mathbf{2.52 \pm 0.06}$ & $\mathbf{2.08 \pm 0.05}$ & $\mathbf{8.5 \pm 0.3}$ & $\mathbf{7.2 \pm 0.3}$ & $\mathbf{9.1 \pm 0.4}$ & $\mathbf{7.5 \pm 0.3}$ \\
% \midrule
% Improv. vs 0-shot & 20.3\% & 20.2\% & 19.2\% & 19.4\% & 24.1\% & 24.2\% & 24.8\% & 23.5\% \\
% Improv. vs FT & 11.3\% & 10.8\% & 10.6\% & 10.3\% & 16.7\% & 16.3\% & 17.3\% & 15.7\% \\
% \bottomrule
% \end{tabular}
% \end{table}

\subsection{Zonal Performance Analysis}\label{sec:zonal_analysis}

Figure~\ref{fig:zonal} visualizes the performance across NYISO's 11 load zones. Table~\ref{tab:zonal_results} provides detailed metrics.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Fig/fig14_zonal_analysis.pdf}
\caption{Zonal performance analysis across NYISO's 11 load zones. (a) Load forecasting MAPE by zone comparing GridFM and Moirai-MoE. Dashed lines indicate system-wide mean. Zone J (NYC) and Zone K (Long Island) show highest MAPE but also largest improvement. (b) Horizontal bar chart of GridFM improvement percentage by zone. The red dashed line indicates mean improvement (18.4\%). All zones show consistent improvement between 17.8\% and 19.2\%.\label{fig:zonal}}
\end{figure}

\begin{table}[H]
\caption{Load forecasting MAPE (\%) by zone.\label{tab:zonal_results}}
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Zone} & \textbf{TFT} & \textbf{Moirai-MoE} & \textbf{GridFM} & \textbf{Improv.} \\
\midrule
A - West & $2.48 \pm 0.05$ & $2.52 \pm 0.06$ & $\mathbf{2.05 \pm 0.04}$ & 18.7\% \\
B - Genesee & $2.52 \pm 0.05$ & $2.58 \pm 0.06$ & $\mathbf{2.12 \pm 0.04}$ & 17.8\% \\
C - Central & $2.50 \pm 0.05$ & $2.55 \pm 0.06$ & $\mathbf{2.08 \pm 0.04}$ & 18.4\% \\
D - North & $2.92 \pm 0.06$ & $2.98 \pm 0.07$ & $\mathbf{2.45 \pm 0.05}$ & 17.8\% \\
E - Mohawk Valley & $2.62 \pm 0.05$ & $2.68 \pm 0.06$ & $\mathbf{2.18 \pm 0.04}$ & 18.7\% \\
F - Capital & $2.58 \pm 0.05$ & $2.62 \pm 0.06$ & $\mathbf{2.15 \pm 0.04}$ & 17.9\% \\
G - Hudson Valley & $2.68 \pm 0.05$ & $2.72 \pm 0.06$ & $\mathbf{2.22 \pm 0.05}$ & 18.4\% \\
H - Millwood & $2.72 \pm 0.06$ & $2.78 \pm 0.06$ & $\mathbf{2.28 \pm 0.05}$ & 18.0\% \\
I - Dunwoodie & $2.82 \pm 0.06$ & $2.88 \pm 0.07$ & $\mathbf{2.35 \pm 0.05}$ & 18.4\% \\
J - New York City & $3.05 \pm 0.07$ & $3.12 \pm 0.07$ & $\mathbf{2.52 \pm 0.05}$ & 19.2\% \\
K - Long Island & $3.22 \pm 0.07$ & $3.28 \pm 0.08$ & $\mathbf{2.68 \pm 0.06}$ & 18.3\% \\
\midrule
\textbf{System Total} & $2.58 \pm 0.06$ & $2.63 \pm 0.06$ & $\mathbf{2.14 \pm 0.05}$ & 18.6\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Study}\label{sec:ablation}

Table~\ref{tab:ablation} and Figure~\ref{fig:ablation} present the ablation study results quantifying the contribution of each GridFM component.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{Fig/fig16_probabilistic_analysis.pdf}
\caption{Ablation study showing the contribution of each GridFM component. Starting from the base Moirai-MoE model, each component is progressively added: FreqMixer adaptation layer, physics-informed constraints, and multi-task learning framework. The full GridFM model achieves the best performance across all tasks. The FreqMixer contributes 6.8\% improvement for load forecasting, physics constraints add 2.9\%, and multi-task learning provides an additional 5.5\%.\label{fig:ablation}}
\end{figure}

\begin{table}[H]
\caption{Ablation study results (MAPE \%). Each row adds one component to the previous configuration.\label{tab:ablation}}
\centering
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Configuration} & \textbf{Load} & \textbf{Price} & \textbf{Emission} & \textbf{Renewable} & \textbf{Params} \\
\midrule
Base Moirai-MoE (0-shot) & $2.63 \pm 0.06$ & $10.15 \pm 0.45$ & $6.05 \pm 0.24$ & $7.92 \pm 0.32$ & 117M \\
+ LoRA Fine-tuning & $2.38 \pm 0.05$ & $9.27 \pm 0.38$ & $5.52 \pm 0.20$ & $7.25 \pm 0.28$ & 119M \\
+ FreqMixer Layer & $2.28 \pm 0.05$ & $8.85 \pm 0.35$ & $5.25 \pm 0.19$ & $6.92 \pm 0.26$ & 125M \\
+ Physics Constraints & $2.22 \pm 0.05$ & $8.52 \pm 0.33$ & $5.02 \pm 0.18$ & $6.65 \pm 0.25$ & 128M \\
+ Multi-Task Heads & $2.18 \pm 0.05$ & $8.12 \pm 0.32$ & $4.85 \pm 0.18$ & $6.42 \pm 0.24$ & 135M \\
+ Coupling Loss & $2.16 \pm 0.05$ & $7.92 \pm 0.31$ & $4.78 \pm 0.18$ & $6.35 \pm 0.24$ & 135M \\
+ Uncertainty Weighting & $\mathbf{2.14 \pm 0.05}$ & $\mathbf{7.80 \pm 0.31}$ & $\mathbf{4.73 \pm 0.18}$ & $\mathbf{6.28 \pm 0.24}$ & 135M \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Component Contributions:}
\begin{itemize}
    \item \textbf{LoRA Fine-tuning:} 9.5\% improvement, establishing a strong baseline
    \item \textbf{FreqMixer:} 4.2\% additional improvement, validating frequency-domain adaptation
    \item \textbf{Physics Constraints:} 2.6\% improvement, with largest impact on emission forecasting
    \item \textbf{Multi-Task Learning:} 1.8\% improvement, with price forecasting benefiting most
\end{itemize}

\subsection{Physics Constraint Effectiveness}\label{sec:physics_results}

Figure~\ref{fig:physics} and Table~\ref{tab:physics_metrics} evaluate the effectiveness of physics-informed constraints.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Fig/fig15_physics_analysis.pdf}
\caption{Physics-informed constraint effectiveness analysis. (a) Power balance constraint violation rate across models. GridFM achieves 67\% reduction compared to Moirai-MoE. (b) Physics and coupling loss convergence during training. (c) Inter-variable correlation preservation comparing true data correlations with model predictions. GridFM closely matches true correlations. (d) Physical consistency metrics showing GridFM achieves $>$92\% compliance across all constraint types.\label{fig:physics}}
\end{figure}

\begin{table}[H]
\caption{Physics constraint effectiveness. Higher is better for compliance metrics ($\uparrow$), lower is better for violations ($\downarrow$).\label{tab:physics_metrics}}
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Metric} & \textbf{TFT} & \textbf{Moirai-MoE} & \textbf{GridFM (no phys.)} & \textbf{GridFM (full)} \\
\midrule
Power Balance Violation (\%) $\downarrow$ & $8.2 \pm 0.5$ & $6.5 \pm 0.4$ & $5.2 \pm 0.3$ & $\mathbf{2.1 \pm 0.2}$ \\
Price-Load Monotonicity (\%) $\uparrow$ & $72.2 \pm 2.1$ & $78.5 \pm 1.8$ & $85.2 \pm 1.5$ & $\mathbf{92.5 \pm 1.2}$ \\
Emission-Renewable Inverse (\%) $\uparrow$ & $68.5 \pm 2.5$ & $76.8 \pm 2.0$ & $82.5 \pm 1.6$ & $\mathbf{94.2 \pm 1.0}$ \\
Ramp Rate Compliance (\%) $\uparrow$ & $75.2 \pm 2.2$ & $82.5 \pm 1.8$ & $88.5 \pm 1.4$ & $\mathbf{96.8 \pm 0.8}$ \\
\bottomrule
\end{tabular}
\end{table}

The physics constraints reduce power balance violations by 67\% (from 6.5\% to 2.1\%) compared to Moirai-MoE.

\subsection{Hyperparameter Sensitivity Analysis}\label{sec:sensitivity}

Table~\ref{tab:sensitivity} presents sensitivity analysis for key hyperparameters.

\begin{table}[H]
\caption{Hyperparameter sensitivity (Load MAPE \%).\label{tab:sensitivity}}
\centering
\small
\begin{tabular}{@{}l|ccccc@{}}
\toprule
\textbf{Parameter} & \textbf{Value 1} & \textbf{Value 2} & \textbf{Default} & \textbf{Value 4} & \textbf{Value 5} \\
\midrule
$\lambda_p$ & 0.01: $2.28 \pm 0.05$ & 0.05: $2.18 \pm 0.05$ & \textbf{0.1: $2.14 \pm 0.05$} & 0.2: $2.16 \pm 0.05$ & 0.5: $2.22 \pm 0.06$ \\
$\lambda_c$ & 0.01: $2.18 \pm 0.05$ & 0.02: $2.16 \pm 0.05$ & \textbf{0.05: $2.14 \pm 0.05$} & 0.1: $2.15 \pm 0.05$ & 0.2: $2.19 \pm 0.05$ \\
LoRA rank $r$ & 4: $2.22 \pm 0.05$ & 8: $2.18 \pm 0.05$ & \textbf{16: $2.14 \pm 0.05$} & 32: $2.14 \pm 0.05$ & 64: $2.15 \pm 0.05$ \\
Context $L$ & 144: $2.25 \pm 0.05$ & \textbf{288: $2.14 \pm 0.05$} & 576: $2.12 \pm 0.05$ & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Probabilistic Forecasting Evaluation}\label{sec:probabilistic_results}

Figure~\ref{fig:probabilistic} and Table~\ref{tab:probabilistic} present probabilistic forecasting metrics.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Fig/fig16_probabilistic_analysis.pdf}
\caption{Probabilistic forecasting performance analysis. (a) Continuous Ranked Probability Score (CRPS) comparison for load and price forecasting. (b) Prediction interval calibration plot showing GridFM achieves near-perfect calibration (close to diagonal). (c) Pinball loss across quantile levels for load forecasting. (d) Prediction interval width by operating condition with corresponding coverage rates. GridFM maintains 95\% target coverage while producing narrower intervals.\label{fig:probabilistic}}
\end{figure}

\begin{table}[H]
\caption{Probabilistic forecasting performance.\label{tab:probabilistic}}
\centering
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Model} & \textbf{CRPS-Load} & \textbf{CRPS-Price} & \textbf{90\% Cov.} & \textbf{95\% Cov.} & \textbf{PI Width} & \textbf{Calib.} \\
\midrule
Chronos & $185 \pm 9$ & $4.2 \pm 0.2$ & 88.5\% & 93.2\% & $425 \pm 18$ & 0.82 \\
Moirai-MoE & $162 \pm 8$ & $3.6 \pm 0.2$ & 89.8\% & 94.5\% & $385 \pm 15$ & 0.87 \\
Chronos-FT & $172 \pm 8$ & $3.8 \pm 0.2$ & 88.8\% & 93.5\% & $405 \pm 15$ & 0.84 \\
Moirai-MoE-FT & $155 \pm 7$ & $3.4 \pm 0.2$ & 89.5\% & 94.2\% & $375 \pm 12$ & 0.88 \\
\textbf{GridFM} & $\mathbf{135 \pm 6}$ & $\mathbf{2.8 \pm 0.1}$ & $\mathbf{90.2\%}$ & $\mathbf{95.2\%}$ & $\mathbf{320 \pm 10}$ & $\mathbf{0.94}$ \\
\midrule
Improv. vs 0-shot & 16.7\% & 22.2\% & +0.4\% & +0.7\% & 16.9\% & +8.0\% \\
Improv. vs FT & 12.9\% & 17.6\% & +0.7\% & +1.0\% & 14.7\% & +6.8\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Computational Efficiency}\label{sec:efficiency}

Figure~\ref{fig:efficiency} and Table~\ref{tab:efficiency} compare computational requirements.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Fig/fig17_efficiency_analysis.pdf}
\caption{Computational efficiency analysis. (a) Inference latency in milliseconds per batch of 64 samples. GridFM (52ms) is comparable to Moirai-MoE (45ms) while achieving better accuracy. (b) Efficiency frontier showing model parameters vs. Load MAPE. GridFM achieves optimal position on the Pareto frontier. (c) GPU memory requirements in GB. Dashed lines indicate common GPU memory thresholds. GridFM requires 9.5GB, compatible with RTX 3090/4080 GPUs.\label{fig:efficiency}}
\end{figure}

\begin{table}[H]
\caption{Computational efficiency comparison.\label{tab:efficiency}}
\centering
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Model} & \textbf{Params (M)} & \textbf{Train (h)} & \textbf{Infer. (ms)} & \textbf{GPU (GB)} & \textbf{FLOPs (G)} \\
\midrule
LSTM & 2.5 & 8 & 12 & 2.1 & 0.8 \\
TFT & 5.2 & 15 & 28 & 4.5 & 2.2 \\
TimesFM & 200 & -- & 85 & 12.5 & 45.2 \\
Chronos-Large & 710 & -- & 120 & 24.0 & 125.5 \\
Moirai-MoE (0-shot) & 117 & -- & 45 & 8.2 & 12.8 \\
Moirai-MoE-FT & 119 & 8 & 45 & 8.5 & 12.8 \\
\textbf{GridFM} & \textbf{135} & \textbf{12} & \textbf{52} & \textbf{9.5} & \textbf{15.2} \\
\bottomrule
\end{tabular}
\begin{flushleft}
\small{*Fine-tuning time only. Zero-shot models require no training.}
\end{flushleft}
\end{table}

GridFM adds modest computational overhead (15\% more parameters, 15\% slower inference) while achieving substantially better performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 6: DISCUSSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}\label{sec:discussion}

This section discusses key findings, limitations, and practical implications.

\subsection{Key Findings}

\subsubsection{Foundation Model Adaptation is Effective}

Our results demonstrate that domain-specific adaptation of general-purpose TSFMs yields substantial improvements. The 10.1\% improvement over fine-tuned Moirai-MoE (18.6\% over zero-shot) validates our hypothesis that power grids require specialized inductive biases beyond what standard fine-tuning provides.

\subsubsection{Physics Constraints Improve Both Accuracy and Consistency}

The 67\% reduction in power balance violations (from 6.5\% to 2.1\%) is significant for grid operators. Physics constraints provide the largest relative contribution to emission forecasting (28.0\% of total improvement), where physical relationships between generation mix and emissions are well-defined.

\subsubsection{Multi-Task Learning Exploits Variable Coupling}

Price forecasting benefits most from the multi-task framework (15.9\% improvement over fine-tuned baseline), as prices are directly influenced by load and renewable generation. The adaptive coupling loss allows the model to learn time-varying relationships rather than enforcing fixed correlations.

\subsubsection{FreqMixer Captures Grid-Specific Patterns}

Analysis of learned frequency masks confirms that FreqMixer emphasizes expected periodicities (12h, 24h, 168h) corresponding to grid operational patterns. The FreqMixer contributes 6.8\% improvement for load forecasting through selective amplification of grid-relevant frequencies.

\subsection{Explainability Analysis}

The SHAP analysis (Figure~\ref{fig:shap}) reveals interpretable feature importance patterns essential for grid operator trust.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Fig/fig9_shap_importance(1).pdf}
\caption{SHAP feature importance analysis for (a) load forecasting and (b) price forecasting. Features are ranked by mean absolute SHAP value. For load forecasting, recent historical load (t-1, t-24) and temperature are most influential. For price forecasting, previous price and hour of day dominate. The interpretable feature attribution supports grid operator decision-making.\label{fig:shap}}
\end{figure}

The attention heatmap (Figure~\ref{fig:attention}) shows GridFM learns to focus on relevant historical patterns.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Fig/fig10_attention_heatmap(1).pdf}
\caption{Attention weight visualization for the load forecasting task. The heatmap shows aggregated attention weights, where rows represent prediction time steps and columns represent context positions. Darker colors indicate higher attention weights. The model learns to focus on recent observations while also attending to the same hour from the previous day (diagonal pattern), capturing daily periodicity.\label{fig:attention}}
\end{figure}

\subsection{Limitations}\label{sec:limitations}

\subsubsection{Computational Requirements}

GridFM requires substantial resources for training (12 hours on 4$\times$A100 GPUs). While inference is efficient (52ms per batch), the training requirements may limit adoption by smaller utilities. Future work could explore more efficient adaptation methods such as adapter layers or prompt tuning.

\subsubsection{Extreme Event Performance}

Table~\ref{tab:extreme_events} shows performance during extreme events remains an area for improvement.

\begin{table}[H]
\caption{Performance during extreme events (Load MAPE \%).\label{tab:extreme_events}}
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Event Type} & \textbf{Samples} & \textbf{Moirai-MoE} & \textbf{GridFM} & \textbf{Improv.} \\
\midrule
Normal Operations & 95.2\% & $2.52 \pm 0.06$ & $\mathbf{2.05 \pm 0.04}$ & 18.7\% \\
Peak Load ($>$95th pctl) & 2.5\% & $4.85 \pm 0.15$ & $\mathbf{4.12 \pm 0.12}$ & 15.1\% \\
Price Spikes ($>3\sigma$) & 1.2\% & $28.5 \pm 1.5$ & $\mathbf{22.8 \pm 1.2}$ & 20.0\% \\
Extreme Weather & 1.1\% & $5.25 \pm 0.18$ & $\mathbf{4.52 \pm 0.15}$ & 13.9\% \\
\bottomrule
\end{tabular}
\end{table}

While GridFM improves extreme event forecasting, performance during price spikes remains challenging (22.8\% MAPE). This reflects the inherent difficulty of predicting rare, high-volatility events. Integration of external signals (e.g., weather forecasts, outage schedules) could improve extreme event detection.

\subsubsection{Geographic Scope}

While we validated on three ISOs (NYISO, PJM, CAISO), these share similar market structures and climate zones. Validation on ISOs with different characteristics (e.g., ERCOT with its isolated grid, or European markets with different regulatory frameworks) would strengthen generalizability claims.

\subsubsection{Temporal Scope}

The test period (2023-2024) may not capture all relevant scenarios, such as major grid failures or unprecedented weather events. The model's behavior under distribution shift from rapid grid evolution (increasing EV penetration, distributed solar) remains to be validated.

\subsubsection{Physics Constraint Limitations}

Our physics constraints use DC power flow approximation, which may not capture AC power flow effects accurately under stressed conditions. The GCN-based topology encoding assumes static grid structure, not accounting for reconfiguration events.

\subsection{Practical Implications}

\subsubsection{Economic Impact}

A 0.5\% reduction in forecast error can save approximately \$10--20 million annually for a large ISO~\cite{Hong2020}. GridFM's 10.1\% improvement over fine-tuned baselines (18.6\% over zero-shot) translates to potential annual savings of \$50--100 million for NYISO-scale operations.

\subsubsection{CLCPA Compliance}

GridFM's emission forecasting capabilities directly support CLCPA 2030/2040 compliance monitoring through real-time carbon accounting and scenario analysis.

\subsubsection{Renewable Integration}

The 20.7\% improvement in renewable forecasting facilitates higher renewable penetration by reducing reserve requirements.

\subsubsection{Deployment Considerations}

For operational deployment, we recommend:
\begin{itemize}
    \item Weekly model retraining with 30-day rolling window
    \item Ensemble predictions combining GridFM with operational persistence models
    \item Automated anomaly detection to flag low-confidence predictions
    \item Human-in-the-loop review for high-stakes decisions
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 7: CONCLUSIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}\label{sec:conclusion}

This paper presented GridFM, a physics-informed foundation model designed specifically for multi-task energy forecasting using real-time data from the New York Independent System Operator, with additional validation on PJM and CAISO datasets. The proposed framework addresses fundamental limitations of existing time series foundation models when applied to power grid applications by introducing domain-specific adaptations that respect the unique characteristics of energy systems.

At the core of GridFM lies the FreqMixer adaptation layer, a novel frequency-domain mixing mechanism that transforms general-purpose foundation model representations into power-grid-specific patterns. By operating in the spectral domain with grid-specific initialization, FreqMixer learns to selectively emphasize frequency components corresponding to characteristic grid periodicities such as daily load cycles, weekly demand patterns, and seasonal variations, achieving a 12.3\% improvement in capturing these essential temporal structures without modifying the pre-trained backbone weights.

The integration of physics-informed constraints represents another significant contribution, embedding fundamental power system laws directly into the learning process. Through the incorporation of power balance equations with DC power flow approximation and zonal topology encoding via graph neural networks, GridFM ensures that predictions respect physical consistency requirements. This approach reduces physically inconsistent predictions by 67\% compared to purely data-driven alternatives, a critical improvement for operational deployment where violated constraints can lead to infeasible dispatch decisions.

The multi-task learning framework with adaptive coupling constraints enables simultaneous forecasting of load demand, locational-based marginal prices, carbon emissions, and renewable generation through a shared representation architecture with task-specific output heads. The uncertainty-weighted loss function automatically balances contributions from each task during training, while the adaptive coupling loss learns time-varying relationships between grid variables. This joint modeling approach exploits the inherent correlations between grid variables, improving individual task performance beyond what single-task models can achieve.

Comprehensive experiments on over 10 years of NYISO data, with rolling-origin cross-validation and statistical significance testing, demonstrate that GridFM achieves statistically significant improvements across all forecasting tasks. The model attains $2.14\% \pm 0.05\%$ MAPE for load forecasting, representing a 10.1\% improvement over fine-tuned Moirai-MoE ($p < 0.01$) and 18.6\% over zero-shot baseline. Price forecasting reaches $7.80\% \pm 0.31\%$ MAPE with a 15.9\% improvement over fine-tuned baseline (23.2\% over zero-shot), while emission prediction achieves $4.73\% \pm 0.18\%$ MAPE, improving by 14.3\% over fine-tuned baseline (21.8\% over zero-shot). These gains are consistent across different forecast horizons, seasons, day types, and geographic zones within the NYISO territory, as well as across PJM and CAISO validation datasets.

The explainability module, incorporating SHAP-based feature attribution and attention visualization, provides interpretable predictions essential for grid operator trust and regulatory compliance. This transparency supports the Climate Leadership and Community Protection Act objectives by enabling real-time carbon accounting and informed decision-making for sustainable energy transition.

Looking ahead, several promising directions emerge from this work. Extension to other ISO regions through transfer learning would further validate the generalizability of the GridFM architecture across different grid topologies and market structures. Integration of conformal prediction methods could provide calibrated uncertainty intervals with formal coverage guarantees. Development of online learning capabilities would enable adaptation to distribution shifts arising from evolving grid composition, including increasing electric vehicle penetration and distributed solar adoption. Finally, exploration of federated learning approaches could enable privacy-preserving model training across multiple utilities without sharing sensitive operational data. These directions collectively point toward a future where foundation models become standard tools for power system operations, contributing to the broader goal of reliable, affordable, and sustainable electricity systems.

\textcolor{red}{A particularly promising avenue for future research is the extension of GridFM to distributed power grids and microgrids. This expansion would require several architectural adaptations: (1) modifying the GCN topology encoding to handle meshed microgrid networks with bidirectional power flows, rather than the primarily radial structure of bulk transmission systems; (2) incorporating behind-the-meter distributed energy resources (DERs) such as rooftop solar, home batteries, and electric vehicles, which introduce additional stochasticity at the distribution level; (3) adapting the physics constraints to account for voltage regulation and reactive power management, which become critical at lower voltage levels; and (4) developing hierarchical forecasting frameworks that coordinate predictions across transmission, distribution, and microgrid levels. Such extensions would enable GridFM to support emerging applications including virtual power plant (VPP) aggregation, community microgrids, peer-to-peer energy trading platforms, and transactive energy markets. The modular architecture of GridFM, with its separable FreqMixer, physics constraint, and multi-task components, provides a flexible foundation for these adaptations.}

%%%%%%%%%%%%%%%%%%%%%%%
% Alternative Bulleted Summary (Commented out, retained from original):
%%%%%%%%%%%%%%%%%%%%%%%
% This paper presented GridFM, a physics-informed foundation model for multi-task energy forecasting using real-time NYISO data. Our key contributions include:

% \begin{enumerate}
%     \item \textbf{FreqMixer Adaptation:} A novel frequency-domain mixing mechanism achieving 12.3\% improvement in capturing grid periodicities.
    
%     \item \textbf{Physics-Informed Constraints:} Integration of power balance equations and zonal topology, reducing physically inconsistent predictions by 67\%.
    
%     \item \textbf{Multi-Task Framework:} Uncertainty-weighted joint forecasting improving individual task performance.
    
%     \item \textbf{State-of-the-Art Performance:} GridFM achieves 18.5\% improvement in load MAPE (2.14\%), 23.2\% in price (7.8\% MAPE), and 21.7\% in emission prediction.
    
%     \item \textbf{Explainability:} SHAP-based feature attribution and attention visualization for interpretable predictions.
% \end{enumerate}

% \subsection{Future Work}

% Several directions warrant further investigation:
% \begin{itemize}
%     \item Extension to other ISO regions with transfer learning
%     \item Integration of probabilistic forecasting with conformal prediction
%     \item Development of real-time streaming deployment pipeline
%     \item Exploration of federated learning for privacy-preserving training
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% AUTHOR CONTRIBUTIONS, FUNDING, ETC.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{6pt}

% FIXED: Corrected author initials to match actual author names (A.S., M.M., S.B.)
\authorcontributions{Conceptualization, A.S. and M.M.; methodology, A.S.; software, A.S.; validation, A.S. and S.B.; formal analysis, A.S.; investigation, A.S.; resources, M.M.; data curation, A.S.; writing---original draft preparation, A.S.; writing---review and editing, S.B. and M.M.; visualization, A.S.; supervision, M.M.; project administration, M.M.; funding acquisition, M.M. All authors have read and agreed to the published version of the manuscript.}

\funding{This research received no external funding.}

\institutionalreview{Not applicable.}

\informedconsent{Not applicable.}

\dataavailability{The NYISO data used in this study are publicly available at \url{https://www.nyiso.com/}. PJM data are available at \url{https://dataminer2.pjm.com/}. CAISO data are available at \url{http://oasis.caiso.com/}}

\conflictsofinterest{The authors declare no conflict of interest.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% NEW REFERENCES TO ADD TO reference.bib (per reviewer suggestions):
% @article{Simeunovic2022TSMGAT,
%   title={Interpretable temporal-spatial graph attention network for multi-site {PV} power forecasting},
%   author={Simeunovi{\'c}, Jelena and Schubnel, Baptiste and Alet, Pierre-Jean and Carrillo, Rafael E. and Frossard, Pascal},
%   journal={Applied Energy},
%   volume={327},
%   pages={120127},
%   year={2022},
%   publisher={Elsevier},
%   doi={10.1016/j.apenergy.2022.120127}
% }
% @article{Zhang2025PIHMTL,
%   title={A Physics-Informed Hybrid Multitask Learning for Lithium-Ion Battery Full-Life Aging Estimation at Early Lifetime},
%   author={Zhang, Shuxin and Liu, Zhitao and Xu, Yan and Su, Hongye},
%   journal={IEEE Transactions on Industrial Informatics},
%   volume={21},
%   number={1},
%   pages={415--424},
%   year={2025},
%   publisher={IEEE},
%   doi={10.1109/TII.2024.3462761}
% }

\reftitle{References}
\bibliography{reference}

\end{document}
